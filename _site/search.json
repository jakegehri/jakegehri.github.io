[
  {
    "objectID": "projects/Counting Cards/counting_cards.html",
    "href": "projects/Counting Cards/counting_cards.html",
    "title": "Counting Cards: Computer vision classifier for playing cards",
    "section": "",
    "text": "import torch\nfrom torch import nn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import make_grid\nfrom torchvision.utils import save_image\nfrom torchvision import datasets\n!pip install torchmetrics\nfrom torchmetrics import Accuracy\n!pip install path\nfrom path import Path\ntry: \n    import torchinfo\nexcept:\n    !pip install torchinfo\n    import torchinfo\n    \nfrom torchinfo import summary\n\nRequirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (0.10.3)\nRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.23.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.12.0+cu116)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.1->torchmetrics) (4.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging->torchmetrics) (3.0.9)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nRequirement already satisfied: path in /usr/local/lib/python3.9/dist-packages (16.5.0)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\ncreds = '{\"username\":\"jakegehri\",\"key\":\"3d0213a52bd1816d21037e941bc77569\"}'\n\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists(): \n    cred_path.parent.mkdir(exist_ok=True) \n    cred_path.write_text(creds) \n    cred_path.chmod(0o600)\n\n\ntrain_dir = Path('train')\nvalid_dir = Path('valid')\ntest_dir = Path('test')\n\n\nimg = (train_dir / 'ace of clubs' / '001.jpg')\nimg = Image.open(img)\nimg\n\n\n\n\n\nimg.size\n\n(224, 224)\n\n\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(64),\n    transforms.RandomVerticalFlip(0.5),\n    transforms.RandomRotation(90),\n    transforms.ToTensor()\n\n])\n\n\ntest_transforms = transforms.Compose([\n    transforms.Resize(64),\n    transforms.ToTensor()\n])\n\n\ntrain_data = datasets.ImageFolder(root=train_dir,\n                                  transform=data_transforms,\n                                  target_transform=None)\n\nvalid_data = datasets.ImageFolder(root=valid_dir,\n                                  transform=data_transforms)\n\ntest_data = datasets.ImageFolder(root=test_dir,\n                                  transform=test_transforms)\n\n\nex = train_data[0][0]\nplt.imshow(ex.permute(1, 2, 0))\n\n<matplotlib.image.AxesImage at 0x7f3aefa8d8b0>\n\n\n\n\n\n\nclass_names = train_data.classes\n\n\nlen(train_data.samples)\n\n7518\n\n\n\ntrain_dl = DataLoader(dataset=train_data,\n                     batch_size=64,\n                     shuffle=True)\n\nvalid_dl = DataLoader(dataset=valid_data,\n                     batch_size=64,\n                     shuffle=False)\n\ntest_dl = DataLoader(dataset=test_data,\n                     batch_size=64,\n                     shuffle=False)\n\n\nSmall VGG Model\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nclass TinyVGG(nn.Module):\n    def __init__(self, input_shape, hidden_units, output_shape):\n        super().__init__()\n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        \n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)   \n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features=hidden_units*16*16, out_features=output_shape)\n        )\n        \n\n    def forward(self, x):\n        x = self.conv_block_1(x)\n        x = self.conv_block_2(x)\n        x = self.classifier(x)\n        return x\n\n\nmodel_0 = TinyVGG(input_shape=3, hidden_units=10, output_shape=len(train_data.classes)).to(device)\n\n\nmodel_0\n\nTinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2560, out_features=52, bias=True)\n  )\n)\n\n\n\nsummary(model_0, input_size=[1, 3, 64, 64])\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nTinyVGG                                  [1, 52]                   --\n├─Sequential: 1-1                        [1, 10, 32, 32]           --\n│    └─Conv2d: 2-1                       [1, 10, 64, 64]           280\n│    └─ReLU: 2-2                         [1, 10, 64, 64]           --\n│    └─Conv2d: 2-3                       [1, 10, 64, 64]           910\n│    └─ReLU: 2-4                         [1, 10, 64, 64]           --\n│    └─MaxPool2d: 2-5                    [1, 10, 32, 32]           --\n├─Sequential: 1-2                        [1, 10, 16, 16]           --\n│    └─Conv2d: 2-6                       [1, 10, 32, 32]           910\n│    └─ReLU: 2-7                         [1, 10, 32, 32]           --\n│    └─Conv2d: 2-8                       [1, 10, 32, 32]           910\n│    └─ReLU: 2-9                         [1, 10, 32, 32]           --\n│    └─MaxPool2d: 2-10                   [1, 10, 16, 16]           --\n├─Sequential: 1-3                        [1, 52]                   --\n│    └─Flatten: 2-11                     [1, 2560]                 --\n│    └─Linear: 2-12                      [1, 52]                   133,172\n==========================================================================================\nTotal params: 136,182\nTrainable params: 136,182\nNon-trainable params: 0\nTotal mult-adds (M): 6.87\n==========================================================================================\nInput size (MB): 0.05\nForward/backward pass size (MB): 0.82\nParams size (MB): 0.54\nEstimated Total Size (MB): 1.41\n==========================================================================================\n\n\n\nimg_batch, label_batch = next(iter(train_dl))\n\nimg_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\nprint(f\"Single image shape: {img_single.shape}\\n\")\n\nmodel_0.eval()\nwith torch.inference_mode():\n    pred = model_0(img_single.to(device))\n\nprint(f\"Output logits:\\n{pred}\\n\")\nprint(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\nprint(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\nprint(f\"Actual label:\\n{label_single}\")\n\nSingle image shape: torch.Size([1, 3, 64, 64])\n\nOutput logits:\ntensor([[-0.0280, -0.0261,  0.0380, -0.0245, -0.0023, -0.0061, -0.0346,  0.0206,\n          0.0212,  0.0360, -0.0263,  0.0042, -0.0558,  0.0214,  0.0385,  0.0275,\n         -0.0160, -0.0569,  0.0222,  0.0187,  0.0538,  0.0149, -0.0417, -0.0173,\n          0.0052, -0.0290,  0.0435,  0.0482, -0.0220, -0.0251, -0.0198,  0.0114,\n          0.0214, -0.0082, -0.0052,  0.0327, -0.0327,  0.0152, -0.0507,  0.0004,\n         -0.0974, -0.0024, -0.0002,  0.0110,  0.0223,  0.0388,  0.0207, -0.0025,\n          0.0742,  0.0031, -0.0055, -0.0171]], device='cuda:0')\n\nOutput prediction probabilities:\ntensor([[0.0187, 0.0187, 0.0200, 0.0188, 0.0192, 0.0191, 0.0186, 0.0196, 0.0196,\n         0.0199, 0.0187, 0.0193, 0.0182, 0.0196, 0.0200, 0.0198, 0.0189, 0.0182,\n         0.0196, 0.0196, 0.0203, 0.0195, 0.0184, 0.0189, 0.0193, 0.0187, 0.0201,\n         0.0202, 0.0188, 0.0187, 0.0188, 0.0194, 0.0196, 0.0191, 0.0191, 0.0199,\n         0.0186, 0.0195, 0.0183, 0.0192, 0.0174, 0.0192, 0.0192, 0.0194, 0.0197,\n         0.0200, 0.0196, 0.0192, 0.0207, 0.0193, 0.0191, 0.0189]],\n       device='cuda:0')\n\nOutput prediction label:\ntensor([48], device='cuda:0')\n\nActual label:\n35\n\n\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               accuracy):\n    \n    model.train()\n    \n    train_loss, train_acc = 0, 0\n\n    for batch, (X, y) in enumerate(dataloader):\n\n        X, y = X.to(device), y.to(device)\n\n        y_pred = model(X)\n\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item() \n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        optimizer.step()\n\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += accuracy(y_pred_class, y)\n\n    train_loss /= len(dataloader)\n    train_acc /= len(dataloader)\n    return train_loss, train_acc\n\n\ndef valid_step(model, dataloader, loss_fn, accuracy):\n    \n    model.eval()\n    \n    valid_loss, valid_acc = 0, 0\n    \n    with torch.no_grad():\n        for batch, (X, y) in enumerate(dataloader):\n            X, y = X.to(device), y.to(device)\n\n            y_pred = model(X)\n            \n            valid_loss += loss_fn(y_pred, y)\n            \n            valid_pred_labels = y_pred.argmax(dim=1)\n            \n            valid_acc += accuracy(valid_pred_labels, y)\n            \n    valid_loss /= len(dataloader)\n    valid_acc /= len(dataloader)\n    \n    return valid_loss, valid_acc\n\n\ndef train(model, train_dataloader, valid_dataloader, optimizer, loss_fn, accuracy, epochs = 5):\n    \n    results = {\"train_loss\": [],\n               \"train_accuracy\": [],\n               \"valid_loss\": [],\n               \"valid_accuracy\": []\n              }\n    \n    for epoch in range(epochs):\n        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, accuracy)\n        \n        valid_loss, valid_acc = valid_step(model, valid_dataloader, loss_fn, accuracy)\n        \n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_accuracy: {train_acc:.4f} | \"\n            f\"valid_loss: {valid_loss:.4f} | \"\n            f\"valid_accuracy: {valid_acc:.4f} | \"\n        )\n        \n        # 5. Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_accuracy\"].append(train_acc)\n        results[\"valid_loss\"].append(valid_loss)\n        results[\"valid_accuracy\"].append(valid_acc)\n        \n    return results\n\n\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\nNUM_EPOCHS = 10\n\n# Recreate an instance of TinyVGG\nmodel_0 = TinyVGG(input_shape=3, hidden_units=10, output_shape=len(train_data.classes)).to(device)\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_0.parameters(), lr=.01)\naccuracy = Accuracy().to(device)\n\nmodel_0_results = train(model_0, train_dl, valid_dl, optimizer, loss_fn, accuracy, epochs=NUM_EPOCHS)\n\nEpoch: 1 | train_loss: 3.9513 | train_accuracy: 0.0199 | valid_loss: 3.9559 | valid_accuracy: 0.0156 | \nEpoch: 2 | train_loss: 3.9487 | train_accuracy: 0.0200 | valid_loss: 3.9526 | valid_accuracy: 0.0156 | \nEpoch: 3 | train_loss: 3.9473 | train_accuracy: 0.0219 | valid_loss: 3.9492 | valid_accuracy: 0.0188 | \nEpoch: 4 | train_loss: 3.9466 | train_accuracy: 0.0215 | valid_loss: 3.9470 | valid_accuracy: 0.0156 | \nEpoch: 5 | train_loss: 3.9462 | train_accuracy: 0.0238 | valid_loss: 3.9452 | valid_accuracy: 0.0156 | \nEpoch: 6 | train_loss: 3.9460 | train_accuracy: 0.0240 | valid_loss: 3.9445 | valid_accuracy: 0.0156 | \nEpoch: 7 | train_loss: 3.9459 | train_accuracy: 0.0240 | valid_loss: 3.9439 | valid_accuracy: 0.0156 | \nEpoch: 8 | train_loss: 3.9459 | train_accuracy: 0.0239 | valid_loss: 3.9436 | valid_accuracy: 0.0156 | \nEpoch: 9 | train_loss: 3.9457 | train_accuracy: 0.0241 | valid_loss: 3.9426 | valid_accuracy: 0.0156 | \nEpoch: 10 | train_loss: 3.9457 | train_accuracy: 0.0240 | valid_loss: 3.9432 | valid_accuracy: 0.0156 | \n\n\n\ntorch.tensor(model_0_results['valid_loss']).cpu().numpy()\n\narray([3.9559014, 3.9525807, 3.949228 , 3.9469597, 3.9452446, 3.9445374,\n       3.9439054, 3.9435685, 3.9426103, 3.9431756], dtype=float32)\n\n\n\ndef plot_loss_curves(results):\n    loss = torch.tensor(results['train_loss']).cpu()\n    valid_loss = torch.tensor(results['valid_loss']).cpu()\n    accuracy = torch.tensor(results['train_accuracy']).cpu()\n    valid_accuracy = torch.tensor(results['valid_accuracy']).cpu()\n\n    # Figure out how many epochs there were\n    epochs = range(len(results['train_loss']))\n\n    # Setup a plot \n    plt.figure(figsize=(15, 7))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label='train_loss')\n    plt.plot(epochs, valid_loss, label='valid_loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, label='train_accuracy')\n    plt.plot(epochs, valid_accuracy, label='valid_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend();\n\n\nplot_loss_curves(model_0_results)\n\n\n\n\n\n\n\nimage.png\n\n\n\n\nTry transfer learning\n\nweights = torchvision.models.EfficientNet_B1_Weights.DEFAULT\nweights\n\nEfficientNet_B1_Weights.IMAGENET1K_V2\n\n\n\nauto_transforms = weights.transforms()\nauto_transforms\n\nImageClassification(\n    crop_size=[240]\n    resize_size=[255]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n\n\n\ntrain_data = datasets.ImageFolder(root=train_dir,\n                                  transform=auto_transforms,\n                                  target_transform=None)\n\nvalid_data = datasets.ImageFolder(root=valid_dir,\n                                  transform=auto_transforms)\n\ntest_data = datasets.ImageFolder(root=test_dir,\n                                  transform=auto_transforms)\n\n\ntrain_data\n\nDataset ImageFolder\n    Number of datapoints: 7518\n    Root location: train\n    StandardTransform\nTransform: ImageClassification(\n               crop_size=[240]\n               resize_size=[255]\n               mean=[0.485, 0.456, 0.406]\n               std=[0.229, 0.224, 0.225]\n               interpolation=InterpolationMode.BILINEAR\n           )\n\n\n\ntrain_dl = DataLoader(dataset=train_data,\n                     batch_size=64,\n                     shuffle=True)\n\nvalid_dl = DataLoader(dataset=valid_data,\n                     batch_size=64,\n                     shuffle=False)\n\ntest_dl = DataLoader(dataset=test_data,\n                     batch_size=64,\n                     shuffle=False)\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel_1 = torchvision.models.efficientnet_b1(weights=weights).to(device)\n\n\nsummary(model_1, input_size=[32, 3, 224, 224], col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"])\n\n===========================================================================================================================================================\nLayer (type:depth-idx)                                  Input Shape               Output Shape              Param #                   Trainable\n===========================================================================================================================================================\nEfficientNet                                            [32, 3, 224, 224]         [32, 1000]                --                        True\n├─Sequential: 1-1                                       [32, 3, 224, 224]         [32, 1280, 7, 7]          --                        True\n│    └─Conv2dNormActivation: 2-1                        [32, 3, 224, 224]         [32, 32, 112, 112]        --                        True\n│    │    └─Conv2d: 3-1                                 [32, 3, 224, 224]         [32, 32, 112, 112]        864                       True\n│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        [32, 32, 112, 112]        64                        True\n│    │    └─SiLU: 3-3                                   [32, 32, 112, 112]        [32, 32, 112, 112]        --                        --\n│    └─Sequential: 2-2                                  [32, 32, 112, 112]        [32, 16, 112, 112]        --                        True\n│    │    └─MBConv: 3-4                                 [32, 32, 112, 112]        [32, 16, 112, 112]        1,448                     True\n│    │    └─MBConv: 3-5                                 [32, 16, 112, 112]        [32, 16, 112, 112]        612                       True\n│    └─Sequential: 2-3                                  [32, 16, 112, 112]        [32, 24, 56, 56]          --                        True\n│    │    └─MBConv: 3-6                                 [32, 16, 112, 112]        [32, 24, 56, 56]          6,004                     True\n│    │    └─MBConv: 3-7                                 [32, 24, 56, 56]          [32, 24, 56, 56]          10,710                    True\n│    │    └─MBConv: 3-8                                 [32, 24, 56, 56]          [32, 24, 56, 56]          10,710                    True\n│    └─Sequential: 2-4                                  [32, 24, 56, 56]          [32, 40, 28, 28]          --                        True\n│    │    └─MBConv: 3-9                                 [32, 24, 56, 56]          [32, 40, 28, 28]          15,350                    True\n│    │    └─MBConv: 3-10                                [32, 40, 28, 28]          [32, 40, 28, 28]          31,290                    True\n│    │    └─MBConv: 3-11                                [32, 40, 28, 28]          [32, 40, 28, 28]          31,290                    True\n│    └─Sequential: 2-5                                  [32, 40, 28, 28]          [32, 80, 14, 14]          --                        True\n│    │    └─MBConv: 3-12                                [32, 40, 28, 28]          [32, 80, 14, 14]          37,130                    True\n│    │    └─MBConv: 3-13                                [32, 80, 14, 14]          [32, 80, 14, 14]          102,900                   True\n│    │    └─MBConv: 3-14                                [32, 80, 14, 14]          [32, 80, 14, 14]          102,900                   True\n│    │    └─MBConv: 3-15                                [32, 80, 14, 14]          [32, 80, 14, 14]          102,900                   True\n│    └─Sequential: 2-6                                  [32, 80, 14, 14]          [32, 112, 14, 14]         --                        True\n│    │    └─MBConv: 3-16                                [32, 80, 14, 14]          [32, 112, 14, 14]         126,004                   True\n│    │    └─MBConv: 3-17                                [32, 112, 14, 14]         [32, 112, 14, 14]         208,572                   True\n│    │    └─MBConv: 3-18                                [32, 112, 14, 14]         [32, 112, 14, 14]         208,572                   True\n│    │    └─MBConv: 3-19                                [32, 112, 14, 14]         [32, 112, 14, 14]         208,572                   True\n│    └─Sequential: 2-7                                  [32, 112, 14, 14]         [32, 192, 7, 7]           --                        True\n│    │    └─MBConv: 3-20                                [32, 112, 14, 14]         [32, 192, 7, 7]           262,492                   True\n│    │    └─MBConv: 3-21                                [32, 192, 7, 7]           [32, 192, 7, 7]           587,952                   True\n│    │    └─MBConv: 3-22                                [32, 192, 7, 7]           [32, 192, 7, 7]           587,952                   True\n│    │    └─MBConv: 3-23                                [32, 192, 7, 7]           [32, 192, 7, 7]           587,952                   True\n│    │    └─MBConv: 3-24                                [32, 192, 7, 7]           [32, 192, 7, 7]           587,952                   True\n│    └─Sequential: 2-8                                  [32, 192, 7, 7]           [32, 320, 7, 7]           --                        True\n│    │    └─MBConv: 3-25                                [32, 192, 7, 7]           [32, 320, 7, 7]           717,232                   True\n│    │    └─MBConv: 3-26                                [32, 320, 7, 7]           [32, 320, 7, 7]           1,563,600                 True\n│    └─Conv2dNormActivation: 2-9                        [32, 320, 7, 7]           [32, 1280, 7, 7]          --                        True\n│    │    └─Conv2d: 3-27                                [32, 320, 7, 7]           [32, 1280, 7, 7]          409,600                   True\n│    │    └─BatchNorm2d: 3-28                           [32, 1280, 7, 7]          [32, 1280, 7, 7]          2,560                     True\n│    │    └─SiLU: 3-29                                  [32, 1280, 7, 7]          [32, 1280, 7, 7]          --                        --\n├─AdaptiveAvgPool2d: 1-2                                [32, 1280, 7, 7]          [32, 1280, 1, 1]          --                        --\n├─Sequential: 1-3                                       [32, 1280]                [32, 1000]                --                        True\n│    └─Dropout: 2-10                                    [32, 1280]                [32, 1280]                --                        --\n│    └─Linear: 2-11                                     [32, 1280]                [32, 1000]                1,281,000                 True\n===========================================================================================================================================================\nTotal params: 7,794,184\nTrainable params: 7,794,184\nNon-trainable params: 0\nTotal mult-adds (G): 18.23\n===========================================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 4786.26\nParams size (MB): 31.18\nEstimated Total Size (MB): 4836.70\n===========================================================================================================================================================\n\n\n\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\noutput_shape = len(train_data.classes)\n\nmodel_1.classifier = nn.Sequential(\n    nn.Dropout(0.2),\n    nn.Linear(in_features=1280, out_features=output_shape, bias=True)\n).to(device)\n\n\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\nNUM_EPOCHS = 5\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)\naccuracy = Accuracy().to(device)\n\nmodel_1_results = train(model_1, train_dl, valid_dl, optimizer, loss_fn, accuracy, epochs=NUM_EPOCHS)\n\nEpoch: 1 | train_loss: 1.5270 | train_accuracy: 0.5790 | valid_loss: 0.3069 | valid_accuracy: 0.9281 | \nEpoch: 2 | train_loss: 0.3717 | train_accuracy: 0.8955 | valid_loss: 0.1809 | valid_accuracy: 0.9313 | \nEpoch: 3 | train_loss: 0.1966 | train_accuracy: 0.9451 | valid_loss: 0.1643 | valid_accuracy: 0.9500 | \nEpoch: 4 | train_loss: 0.1269 | train_accuracy: 0.9658 | valid_loss: 0.0710 | valid_accuracy: 0.9875 | \nEpoch: 5 | train_loss: 0.0942 | train_accuracy: 0.9730 | valid_loss: 0.0777 | valid_accuracy: 0.9750 | \nEpoch: 6 | train_loss: 0.0873 | train_accuracy: 0.9751 | valid_loss: 0.1236 | valid_accuracy: 0.9781 | \nEpoch: 7 | train_loss: 0.0633 | train_accuracy: 0.9819 | valid_loss: 0.0655 | valid_accuracy: 0.9781 | \nEpoch: 8 | train_loss: 0.0530 | train_accuracy: 0.9850 | valid_loss: 0.0785 | valid_accuracy: 0.9844 | \nEpoch: 9 | train_loss: 0.0493 | train_accuracy: 0.9845 | valid_loss: 0.1224 | valid_accuracy: 0.9313 | \nEpoch: 10 | train_loss: 0.0466 | train_accuracy: 0.9864 | valid_loss: 0.2084 | valid_accuracy: 0.9125 | \n\n\n\nplot_loss_curves(model_1_results)\n\n\n\n\n\n\nWith better transforms\n\naug_transforms = transforms.Compose([\n    transforms.TrivialAugmentWide(20),\n    weights.transforms()\n])\n\n\ntrain_data = datasets.ImageFolder(root=train_dir,\n                                  transform=aug_transforms,\n                                  target_transform=None)\n\nvalid_data = datasets.ImageFolder(root=valid_dir,\n                                  transform=aug_transforms)\n\ntest_data = datasets.ImageFolder(root=test_dir,\n                                  transform=aug_transforms)\n\n\nex = train_data[0][0]\nplt.imshow(ex.permute(1, 2, 0))\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n<matplotlib.image.AxesImage at 0x7f3ae1af8b50>\n\n\n\n\n\n\ntrain_dl = DataLoader(dataset=train_data,\n                     batch_size=64,\n                     shuffle=True)\n\nvalid_dl = DataLoader(dataset=valid_data,\n                     batch_size=64,\n                     shuffle=False)\n\ntest_dl = DataLoader(dataset=test_data,\n                     batch_size=64,\n                     shuffle=False)\n\n\nmodel_2 = torchvision.models.efficientnet_b1(weights=weights).to(device)\n\n\nsummary(model_2, input_size= (32, 3, 224, 224), col_names= [\"input_size\", \"output_size\", \"num_params\", \"trainable\"])\n\n===========================================================================================================================================================\nLayer (type:depth-idx)                                  Input Shape               Output Shape              Param #                   Trainable\n===========================================================================================================================================================\nEfficientNet                                            [32, 3, 224, 224]         [32, 1000]                --                        True\n├─Sequential: 1-1                                       [32, 3, 224, 224]         [32, 1280, 7, 7]          --                        True\n│    └─Conv2dNormActivation: 2-1                        [32, 3, 224, 224]         [32, 32, 112, 112]        --                        True\n│    │    └─Conv2d: 3-1                                 [32, 3, 224, 224]         [32, 32, 112, 112]        864                       True\n│    │    └─BatchNorm2d: 3-2                            [32, 32, 112, 112]        [32, 32, 112, 112]        64                        True\n│    │    └─SiLU: 3-3                                   [32, 32, 112, 112]        [32, 32, 112, 112]        --                        --\n│    └─Sequential: 2-2                                  [32, 32, 112, 112]        [32, 16, 112, 112]        --                        True\n│    │    └─MBConv: 3-4                                 [32, 32, 112, 112]        [32, 16, 112, 112]        1,448                     True\n│    │    └─MBConv: 3-5                                 [32, 16, 112, 112]        [32, 16, 112, 112]        612                       True\n│    └─Sequential: 2-3                                  [32, 16, 112, 112]        [32, 24, 56, 56]          --                        True\n│    │    └─MBConv: 3-6                                 [32, 16, 112, 112]        [32, 24, 56, 56]          6,004                     True\n│    │    └─MBConv: 3-7                                 [32, 24, 56, 56]          [32, 24, 56, 56]          10,710                    True\n│    │    └─MBConv: 3-8                                 [32, 24, 56, 56]          [32, 24, 56, 56]          10,710                    True\n│    └─Sequential: 2-4                                  [32, 24, 56, 56]          [32, 40, 28, 28]          --                        True\n│    │    └─MBConv: 3-9                                 [32, 24, 56, 56]          [32, 40, 28, 28]          15,350                    True\n│    │    └─MBConv: 3-10                                [32, 40, 28, 28]          [32, 40, 28, 28]          31,290                    True\n│    │    └─MBConv: 3-11                                [32, 40, 28, 28]          [32, 40, 28, 28]          31,290                    True\n│    └─Sequential: 2-5                                  [32, 40, 28, 28]          [32, 80, 14, 14]          --                        True\n│    │    └─MBConv: 3-12                                [32, 40, 28, 28]          [32, 80, 14, 14]          37,130                    True\n│    │    └─MBConv: 3-13                                [32, 80, 14, 14]          [32, 80, 14, 14]          102,900                   True\n│    │    └─MBConv: 3-14                                [32, 80, 14, 14]          [32, 80, 14, 14]          102,900                   True\n│    │    └─MBConv: 3-15                                [32, 80, 14, 14]          [32, 80, 14, 14]          102,900                   True\n│    └─Sequential: 2-6                                  [32, 80, 14, 14]          [32, 112, 14, 14]         --                        True\n│    │    └─MBConv: 3-16                                [32, 80, 14, 14]          [32, 112, 14, 14]         126,004                   True\n│    │    └─MBConv: 3-17                                [32, 112, 14, 14]         [32, 112, 14, 14]         208,572                   True\n│    │    └─MBConv: 3-18                                [32, 112, 14, 14]         [32, 112, 14, 14]         208,572                   True\n│    │    └─MBConv: 3-19                                [32, 112, 14, 14]         [32, 112, 14, 14]         208,572                   True\n│    └─Sequential: 2-7                                  [32, 112, 14, 14]         [32, 192, 7, 7]           --                        True\n│    │    └─MBConv: 3-20                                [32, 112, 14, 14]         [32, 192, 7, 7]           262,492                   True\n│    │    └─MBConv: 3-21                                [32, 192, 7, 7]           [32, 192, 7, 7]           587,952                   True\n│    │    └─MBConv: 3-22                                [32, 192, 7, 7]           [32, 192, 7, 7]           587,952                   True\n│    │    └─MBConv: 3-23                                [32, 192, 7, 7]           [32, 192, 7, 7]           587,952                   True\n│    │    └─MBConv: 3-24                                [32, 192, 7, 7]           [32, 192, 7, 7]           587,952                   True\n│    └─Sequential: 2-8                                  [32, 192, 7, 7]           [32, 320, 7, 7]           --                        True\n│    │    └─MBConv: 3-25                                [32, 192, 7, 7]           [32, 320, 7, 7]           717,232                   True\n│    │    └─MBConv: 3-26                                [32, 320, 7, 7]           [32, 320, 7, 7]           1,563,600                 True\n│    └─Conv2dNormActivation: 2-9                        [32, 320, 7, 7]           [32, 1280, 7, 7]          --                        True\n│    │    └─Conv2d: 3-27                                [32, 320, 7, 7]           [32, 1280, 7, 7]          409,600                   True\n│    │    └─BatchNorm2d: 3-28                           [32, 1280, 7, 7]          [32, 1280, 7, 7]          2,560                     True\n│    │    └─SiLU: 3-29                                  [32, 1280, 7, 7]          [32, 1280, 7, 7]          --                        --\n├─AdaptiveAvgPool2d: 1-2                                [32, 1280, 7, 7]          [32, 1280, 1, 1]          --                        --\n├─Sequential: 1-3                                       [32, 1280]                [32, 1000]                --                        True\n│    └─Dropout: 2-10                                    [32, 1280]                [32, 1280]                --                        --\n│    └─Linear: 2-11                                     [32, 1280]                [32, 1000]                1,281,000                 True\n===========================================================================================================================================================\nTotal params: 7,794,184\nTrainable params: 7,794,184\nNon-trainable params: 0\nTotal mult-adds (G): 18.23\n===========================================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 4786.26\nParams size (MB): 31.18\nEstimated Total Size (MB): 4836.70\n===========================================================================================================================================================\n\n\n\nmodel_2.classifier = nn.Sequential(\n    nn.Dropout(p=0.2, inplace=True),\n    nn.Linear(1280, out_features=len(train_data.classes), bias=True)\n).to(device)\n\n\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\nNUM_EPOCHS = 5\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_2.parameters(), lr=0.001)\naccuracy = Accuracy().to(device)\n\nmodel_2_results = train(model_2, train_dl, valid_dl, optimizer, loss_fn, accuracy, epochs=NUM_EPOCHS)\n\nEpoch: 1 | train_loss: 1.8372 | train_accuracy: 0.4671 | valid_loss: 0.8242 | valid_accuracy: 0.7656 | \nEpoch: 2 | train_loss: 0.6671 | train_accuracy: 0.8002 | valid_loss: 0.2849 | valid_accuracy: 0.9281 | \nEpoch: 3 | train_loss: 0.4505 | train_accuracy: 0.8617 | valid_loss: 0.2810 | valid_accuracy: 0.8750 | \nEpoch: 4 | train_loss: 0.3458 | train_accuracy: 0.8985 | valid_loss: 0.2187 | valid_accuracy: 0.9563 | \nEpoch: 5 | train_loss: 0.2995 | train_accuracy: 0.9148 | valid_loss: 0.1778 | valid_accuracy: 0.9625 | \n\n\n\nplot_loss_curves(model_2_results)\n\n\n\n\n\nfrom typing import List, Tuple\n\nfrom PIL import Image\n\ndef pred_and_plot_image(model: torch.nn.Module,\n                        image_path: str, \n                        class_names: List[str],\n                        image_size: Tuple[int, int] = (224, 224),\n                        transform: torchvision.transforms = None,\n                        device: torch.device=device):\n    \n    img = Image.open(image_path)\n\n    if transform is not None:\n        image_transform = transform\n    else:\n        image_transform = transforms.Compose([\n            transforms.Resize(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225]),\n        ])\n\n    model.to(device)\n\n    model.eval()\n    with torch.inference_mode():\n        transformed_image = image_transform(img).unsqueeze(dim=0)\n\n        target_image_pred = model(transformed_image.to(device))\n\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n\n    plt.figure()\n    plt.imshow(img)\n    plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n    plt.axis(False);\n\n\n# Get a random list of image paths from test set\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data \ntest_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n\n# Make predictions on and plot the images\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=model_2, \n                        image_path=image_path,\n                        class_names=class_names,\n                        # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n                        image_size=(224, 224))"
  },
  {
    "objectID": "projects/Syllogism Classification/syllogism_classification.html",
    "href": "projects/Syllogism Classification/syllogism_classification.html",
    "title": "Syllogism Validation with BERT",
    "section": "",
    "text": "import pandas as pd\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport transformers\nfrom transformers import DistilBertTokenizer\nfrom transformers import DistilBertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import load_metric\nimport numpy as np\n\n\ndf = pd.read_csv('Avicenna_Train.csv', encoding='ISO-8859-1')\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Premise 1\n      Premise 2\n      Syllogistic relation\n      Conclusion\n    \n  \n  \n    \n      0\n      unchecked imbalances in the society, will see ...\n      correct these imbalances requires in-depth kno...\n      no\n      No conclusion\n    \n    \n      1\n      Chronic diseases are heart attacks and stroke,...\n      In populations that eat a regular high-fiber d...\n      yes\n      In populations that eat a regular high-fiber d...\n    \n    \n      2\n      Formative assessment encourages children to en...\n      An ideal learning environment uses formative a...\n      yes\n      An ideal learning environment encourages child...\n    \n    \n      3\n      Underrepresented female labor force in some pr...\n      Job discrimination comes with underrepresented...\n      yes\n      Job discrimination comes with not being able t...\n    \n    \n      4\n      damaged mentality in an individual brings seri...\n      Aggression harms the mentality of person.\n      yes\n      Aggression brings brings serious health proble...\n    \n  \n\n\n\n\n\ndf['label'] = df['Syllogistic relation'].eq('yes').mul(1)\n\n\ndf['text'] = (df['Premise 1'] + \" : \" + df['Premise 2'])\n\n\ndf['label'].value_counts()\n\n1    2427\n0    2373\nName: label, dtype: int64\n\n\n\nint(len(df) * 0.8)\n\n3840\n\n\n\ntrain_texts = df.iloc[:3840]['text'].values\ntrain_labels = df.iloc[:3840]['label'].values\n\nvalid_texts = df.iloc[3840:]['text'].values\nvalid_labels = df.iloc[3840:]['label'].values\n\n\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n\n\n\n\n\n\n\n\n\n\ntrain_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\nvalid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True)\n\n\nclass SyllogismDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n    \n    def __len__(self):\n        return len(self.labels)\n\n\ntrain_dataset = SyllogismDataset(train_encodings, train_labels)\nvalid_dataset = SyllogismDataset(valid_encodings, valid_labels)\n\n\ntrain_dataloader = torch.utils.data.DataLoader2(train_dataset, batch_size=16, shuffle=True)\nvalid_dataloader = torch.utils.data.DataLoader2(valid_dataset, batch_size=16, shuffle=True)\n\n\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n\n\n\n\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nDEVICE = 'cuda'\n\n\nmodel.train()\n\nmetrics = load_metric('accuracy')\n\n\n\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    \n    predictions = np.argmax(logits, axis=-1)\n    return metrics.compute(predictions=predictions, references=labels)\n\n\ntraining_args = TrainingArguments(output_dir='./results', num_train_epochs=3, per_device_train_batch_size=16,\n                                 per_device_eval_batch_size=16, logging_dir='./logs', logging_steps=72)\n\ntrainer = Trainer(model=model, \n                  args=training_args, \n                  train_dataset=train_dataset, \n                  eval_dataset=valid_dataset,\n                  compute_metrics=compute_metrics\n                 )\n\n\ntrainer.train()\n\n/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 3840\n  Num Epochs = 3\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 720\n\n\n\n\n    \n      \n      \n      [720/720 01:36, Epoch 3/3]\n    \n    \n  \n \n      Step\n      Training Loss\n    \n  \n  \n    \n      72\n      0.658500\n    \n    \n      144\n      0.492300\n    \n    \n      216\n      0.413400\n    \n    \n      288\n      0.298300\n    \n    \n      360\n      0.253200\n    \n    \n      432\n      0.216700\n    \n    \n      504\n      0.178600\n    \n    \n      576\n      0.106900\n    \n    \n      648\n      0.106800\n    \n    \n      720\n      0.091800\n    \n  \n\n\n\nSaving model checkpoint to ./results/checkpoint-500\nConfiguration saved in ./results/checkpoint-500/config.json\nModel weights saved in ./results/checkpoint-500/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\nTrainOutput(global_step=720, training_loss=0.281636557314131, metrics={'train_runtime': 97.56, 'train_samples_per_second': 118.081, 'train_steps_per_second': 7.38, 'total_flos': 289110097566720.0, 'train_loss': 0.281636557314131, 'epoch': 3.0})\n\n\n\ntrainer.evaluate()\n\n***** Running Evaluation *****\n  Num examples = 960\n  Batch size = 16\n\n\n\n    \n      \n      \n      [60/60 02:07]\n    \n    \n\n\n{'eval_loss': 0.4387502670288086,\n 'eval_accuracy': 0.88125,\n 'eval_runtime': 2.2301,\n 'eval_samples_per_second': 430.476,\n 'eval_steps_per_second': 26.905,\n 'epoch': 3.0}\n\n\n\ndf_test = pd.read_csv('Avicenna_Test.csv', encoding='ISO-8859-1')\n\ndf_test['label'] = df_test['Syllogistic relation'].eq('yes').mul(1)\ndf_test['text'] = (df_test['Premise 1'] + \" : \" + df_test['Premise 2'])\n\ntest_texts = df_test['text'].values\ntest_labels = df_test['label'].values\n\ntest_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\n\ntest_dataset = SyllogismDataset(test_encodings, test_labels)\n\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=True)\n\n\ntrainer.evaluate(test_dataset)\n\n***** Running Evaluation *****\n  Num examples = 1200\n  Batch size = 16\n\n\n{'eval_loss': 0.5759531855583191,\n 'eval_accuracy': 0.8525,\n 'eval_runtime': 2.8515,\n 'eval_samples_per_second': 420.837,\n 'eval_steps_per_second': 26.302,\n 'epoch': 3.0}\n\n\n\nsample_text = ['Socrates is a man : all men are mortal']\nsample_label = [1]\n\n\nsample_encoded = tokenizer(sample_text, truncation=True, padding=True)\n\n\nsample_dataset = SyllogismDataset(sample_encoded, sample_label)\nsample_dataset\n\n<__main__.SyllogismDataset at 0x7f63a4fccd60>\n\n\n\ntrainer.predict(sample_dataset).label_ids\n\n***** Running Prediction *****\n  Num examples = 1\n  Batch size = 16\n\n\narray([1])\n\n\n\nsample_text_2 = ['If the streets are wet, it has rained recently : The streets are wet.']\nsample_label_2 = [0]\n\nsample_encoded_2 = tokenizer(sample_text_2, truncation=True, padding=True)\n\nsample_dataset_2 = SyllogismDataset(sample_encoded_2, sample_label_2)\n\ntrainer.predict(sample_dataset_2).label_ids\n\n***** Running Prediction *****\n  Num examples = 1\n  Batch size = 16\n\n\narray([0])"
  },
  {
    "objectID": "projects/Amazon/amazon.html",
    "href": "projects/Amazon/amazon.html",
    "title": "Multi-label Classification of Amazonian Land Space",
    "section": "",
    "text": "import pandoc\nfrom fastbook import *\nfrom fastai import *\n\n\ncreds = '{\"username\":\"jakegehri\",\"key\":\"3d0213a52bd1816d21037e941bc77569\"}'\n\n\n# Creates a cred path for kaggle datasets to be downloaded in comand line\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser() \n\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\npath = Path('./planet/planet')\n\n\npath.ls()\n\n(#4) [Path('planet/planet/train_classes.csv'),Path('planet/planet/test-jpg'),Path('planet/planet/sample_submission.csv'),Path('planet/planet/train-jpg')]\n\n\n\ntrain = (path/'train-jpg').ls()\n\n\ntrain\n\n(#40479) [Path('planet/planet/train-jpg/train_19921.jpg'),Path('planet/planet/train-jpg/train_24619.jpg'),Path('planet/planet/train-jpg/train_21510.jpg'),Path('planet/planet/train-jpg/train_31089.jpg'),Path('planet/planet/train-jpg/train_33277.jpg'),Path('planet/planet/train-jpg/train_11172.jpg'),Path('planet/planet/train-jpg/train_14671.jpg'),Path('planet/planet/train-jpg/train_29521.jpg'),Path('planet/planet/train-jpg/train_27535.jpg'),Path('planet/planet/train-jpg/train_13323.jpg')...]\n\n\n\ndata = (path/'train_classes.csv')\n\n\ndf = pd.read_csv(data)\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      image_name\n      tags\n    \n  \n  \n    \n      0\n      train_0\n      haze primary\n    \n    \n      1\n      train_1\n      agriculture clear primary water\n    \n    \n      2\n      train_2\n      clear primary\n    \n    \n      3\n      train_3\n      clear primary\n    \n    \n      4\n      train_4\n      agriculture clear habitation primary road\n    \n  \n\n\n\n\n\ndblock = DataBlock()\n\n\ndsets = dblock.datasets(df)\n\n\ndsets[0]\n\n(image_name         train_0\n tags          haze primary\n Name: 0, dtype: object,\n image_name         train_0\n tags          haze primary\n Name: 0, dtype: object)\n\n\n\ndef get_x(r): return path/'train-jpg'/(r['image_name'] + '.jpg')\ndef get_y(r): return r['tags'].split(' ')\n\n\ndblock = DataBlock(get_x=get_x, get_y=get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n(Path('planet/planet/train-jpg/train_1398.jpg'),\n ['agriculture', 'partly_cloudy', 'primary', 'road'])\n\n\n\ndblock = DataBlock(blocks = (ImageBlock, MultiCategoryBlock), \n                   get_x=get_x, get_y=get_y)\n\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n(PILImage mode=RGB size=256x256,\n TensorMultiCategory([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.]))\n\n\n\ndblock = DataBlock(blocks = (ImageBlock, MultiCategoryBlock), \n                   splitter=RandomSplitter(seed=42),\n                   get_x=get_x, get_y=get_y)\n\ndsets = dblock.datasets(df)\n\n\nlen(dsets.train), len(dsets.valid)\n\n(32384, 8095)\n\n\n\ndblock = DataBlock(blocks = (ImageBlock, MultiCategoryBlock), \n                   splitter=RandomSplitter(seed=42),\n                   get_x=get_x, get_y=get_y,\n                   item_tfms=RandomResizedCrop(128, min_scale=0.35)\n                  \n)\n\ndls = dblock.dataloaders(df)\n\n\ndls.show_batch()\n\n\n\n\n\nExperiment 1\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy_multi)\nlearn.fine_tune(6, freeze_epochs=2)\nlearn.recorder.plot_loss()\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.242502\n      0.150174\n      0.945827\n      00:25\n    \n    \n      1\n      0.147816\n      0.126972\n      0.951968\n      00:20\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.127971\n      0.109930\n      0.957991\n      00:23\n    \n    \n      1\n      0.119986\n      0.104264\n      0.960476\n      00:23\n    \n    \n      2\n      0.109712\n      0.098336\n      0.962301\n      00:38\n    \n    \n      3\n      0.104570\n      0.097734\n      0.962555\n      00:49\n    \n  \n\n\n\n\n\nExperiment 2\n\nlearn2 = vision_learner(dls, resnet34, metrics=accuracy_multi)\nlearn2.fine_tune(6, freeze_epochs=2)\nlearn2.recorder.plot_loss()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.244604\n      0.147677\n      0.947447\n      00:51\n    \n    \n      1\n      0.148016\n      0.123402\n      0.953312\n      00:25\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.123248\n      0.106454\n      0.959452\n      00:59\n    \n    \n      1\n      0.118266\n      0.104211\n      0.960942\n      00:34\n    \n    \n      2\n      0.111732\n      0.098766\n      0.962381\n      00:42\n    \n    \n      3\n      0.105815\n      0.096162\n      0.963245\n      00:34\n    \n    \n      4\n      0.099851\n      0.094583\n      0.964037\n      00:34\n    \n    \n      5\n      0.093574\n      0.095035\n      0.963848\n      00:34\n    \n  \n\n\n\n\n\n\n\n\nExperiment 3\n\nlearn3 = vision_learner(dls, resnet34, metrics=partial(accuracy_multi, thresh=0.2))\nlearn3.fine_tune(6, freeze_epochs=2)\nlearn3.recorder.plot_loss()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.240184\n      0.151341\n      0.934862\n      00:38\n    \n    \n      1\n      0.151022\n      0.122403\n      0.939382\n      00:27\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.124375\n      0.106883\n      0.950107\n      00:35\n    \n    \n      1\n      0.118021\n      0.101975\n      0.951757\n      00:34\n    \n    \n      2\n      0.109789\n      0.099863\n      0.952055\n      00:34\n    \n    \n      3\n      0.104578\n      0.096811\n      0.951851\n      00:34\n    \n    \n      4\n      0.095859\n      0.095459\n      0.953784\n      00:34\n    \n    \n      5\n      0.092178\n      0.095330\n      0.954445\n      01:02\n    \n  \n\n\n\n\n\n\n\n\nExperiment 4\n\nlearn4 = vision_learner(dls, resnet34, metrics=partial(accuracy_multi, thresh=0.7))\nlearn4.fine_tune(6, freeze_epochs=2)\nlearn4.recorder.plot_loss()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.248779\n      0.148406\n      0.942892\n      00:52\n    \n    \n      1\n      0.147272\n      0.123920\n      0.950325\n      00:37\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.125626\n      0.109230\n      0.955863\n      00:48\n    \n    \n      1\n      0.118446\n      0.103957\n      0.956015\n      00:34\n    \n    \n      2\n      0.112594\n      0.099723\n      0.958754\n      00:37\n    \n    \n      3\n      0.106374\n      0.096328\n      0.958689\n      00:51\n    \n    \n      4\n      0.099089\n      0.094283\n      0.960499\n      00:34\n    \n    \n      5\n      0.095276\n      0.094695\n      0.961879\n      00:39\n    \n  \n\n\n\n\n\n\n\ntest_dl = learn2.dls.test_dl(get_image_files(path/'test-jpg'))\ntest_dl.show_batch()\n\n\n\n\n\npreds, _ = learn2.get_preds(dl=test_dl)\n\n\n\n\n\n\n\n\n\nthresh = 0.5\nlabelled_preds = [' '.join([learn2.dls.vocab[i] for i,p in enumerate(pred) if p > thresh]) for pred in preds]\n\n\nlabelled_preds[:5]\n\n['agriculture clear primary road',\n 'bare_ground clear',\n 'clear primary',\n 'clear primary',\n 'clear primary']\n\n\n\nfnames = []\n\nfor name in os.listdir((path/'test-jpg')):\n    fnames.append(name)\n\n\nfnames[0:5]\n\n['test_36099.jpg',\n 'test_27503.jpg',\n 'test_15453.jpg',\n 'test_20695.jpg',\n 'test_5439.jpg']\n\n\n\nlearn2.predict((path/'test-jpg'/(fnames[0])))\n\n\n\n\n\n\n\n\n((#4) ['agriculture','clear','primary','road'],\n TensorBase([ True, False, False, False, False,  True, False, False, False, False, False, False,  True,  True, False, False, False]),\n TensorBase([9.0538e-01, 3.7246e-04, 4.8710e-02, 5.2741e-04, 2.3139e-04, 9.9834e-01, 1.1549e-05, 2.1592e-04, 2.8083e-01, 2.6772e-01, 1.3102e-03, 5.9414e-04, 9.9820e-01, 7.1026e-01, 3.3888e-03,\n             1.9953e-02, 1.1821e-01]))\n\n\n\nsample = pd.read_csv(path/'sample_submission.csv')\nsample\n\n\n\n\n\n  \n    \n      \n      image_name\n      tags\n    \n  \n  \n    \n      0\n      test_0\n      primary clear agriculture road water\n    \n    \n      1\n      test_1\n      primary clear agriculture road water\n    \n    \n      2\n      test_2\n      primary clear agriculture road water\n    \n    \n      3\n      test_3\n      primary clear agriculture road water\n    \n    \n      4\n      test_4\n      primary clear agriculture road water\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      61186\n      file_9995\n      primary clear agriculture road water\n    \n    \n      61187\n      file_9996\n      primary clear agriculture road water\n    \n    \n      61188\n      file_9997\n      primary clear agriculture road water\n    \n    \n      61189\n      file_9998\n      primary clear agriculture road water\n    \n    \n      61190\n      file_9999\n      primary clear agriculture road water\n    \n  \n\n61191 rows × 2 columns\n\n\n\n\ndf = pd.DataFrame({'image_name':fnames, 'tags':labelled_preds}, columns=['image_name', 'tags'])\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      image_name\n      tags\n    \n  \n  \n    \n      0\n      test_36099.jpg\n      agriculture clear primary road\n    \n    \n      1\n      test_27503.jpg\n      bare_ground clear\n    \n    \n      2\n      test_15453.jpg\n      clear primary\n    \n    \n      3\n      test_20695.jpg\n      clear primary\n    \n    \n      4\n      test_5439.jpg\n      clear primary\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      40664\n      test_12322.jpg\n      clear primary water\n    \n    \n      40665\n      test_10596.jpg\n      agriculture partly_cloudy primary road\n    \n    \n      40666\n      test_567.jpg\n      partly_cloudy primary\n    \n    \n      40667\n      test_23428.jpg\n      agriculture clear primary\n    \n    \n      40668\n      test_10099.jpg\n      clear primary\n    \n  \n\n40669 rows × 2 columns"
  },
  {
    "objectID": "projects/Twitter Bot/model.html",
    "href": "projects/Twitter Bot/model.html",
    "title": "Twitter Bot: NLP Emotion Classifier",
    "section": "",
    "text": "! huggingface-cli login\n\n\n! pip install datasets\nfrom datasets import list_datasets\nimport tensorflow as tf\nfrom transformers import pipeline, PushToHubCallback\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.5.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\nRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\nRequirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\nRequirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\nRequirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\nRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n\n\n\nall_datasets = list_datasets()\nprint(all_datasets[0:5])\n\n['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus']\n\n\n\nfrom datasets import load_dataset\n\n\nemotions = load_dataset('emotion')\n\n/usr/local/lib/python3.7/dist-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n  warnings.warn(message, FutureWarning)\n\n\n\n\n\n\n\n\nWARNING:datasets.builder:Using custom data configuration default\n\n\nDownloading and preparing dataset emotion/default (download: 1.97 MiB, generated: 2.07 MiB, post-processed: Unknown size, total: 4.05 MiB) to /root/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset emotion downloaded and prepared to /root/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705. Subsequent calls will reuse this data.\n\n\n\n\n\n\ntrain_ds = emotions['train']\ntrain_ds\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 16000\n})\n\n\n\ntrain_ds[0]\n\n{'text': 'i didnt feel humiliated', 'label': 0}\n\n\n\nimport pandas as pd\n\n\nemotions.set_format(type = 'pandas')\n\n\ndf = emotions['train'][:]\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      text\n      label\n    \n  \n  \n    \n      0\n      i didnt feel humiliated\n      0\n    \n    \n      1\n      i can go from feeling so hopeless to so damned...\n      0\n    \n    \n      2\n      im grabbing a minute to post i feel greedy wrong\n      3\n    \n    \n      3\n      i am ever feeling nostalgic about the fireplac...\n      2\n    \n    \n      4\n      i am feeling grouchy\n      3\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      15995\n      i just had a very brief time in the beanbag an...\n      0\n    \n    \n      15996\n      i am now turning and i feel pathetic that i am...\n      0\n    \n    \n      15997\n      i feel strong and good overall\n      1\n    \n    \n      15998\n      i feel like this was such a rude comment and i...\n      3\n    \n    \n      15999\n      i know a lot but i feel so stupid because i ca...\n      0\n    \n  \n\n16000 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndef label_int2str(row):\n    return emotions['train'].features['label'].int2str(row)\ndf['label_name'] = df['label'].apply(label_int2str)\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      text\n      label\n      label_name\n    \n  \n  \n    \n      0\n      i didnt feel humiliated\n      0\n      sadness\n    \n    \n      1\n      i can go from feeling so hopeless to so damned...\n      0\n      sadness\n    \n    \n      2\n      im grabbing a minute to post i feel greedy wrong\n      3\n      anger\n    \n    \n      3\n      i am ever feeling nostalgic about the fireplac...\n      2\n      love\n    \n    \n      4\n      i am feeling grouchy\n      3\n      anger\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nimport matplotlib.pyplot as plt\n\ndf['label_name'].value_counts(ascending=True).plot.barh()\nplt.title(\"Frequency of Classes\")\nplt.show()\n\n\n\n\n\ndf['words_per_tweet'] = df['text'].str.split().apply(len)\ndf.boxplot('words_per_tweet', by='label_name', grid=False, showfliers=False)\nplt.suptitle(\"\")\nplt.xlabel(\"\")\nplt.show()\n\n/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X))\n\n\n\n\n\n\nemotions.reset_format()\n\n\n! pip install transformers\nfrom transformers import AutoTokenizer\n\nmodel_checkpoint = 'distilbert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting transformers\n  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n     |████████████████████████████████| 4.9 MB 34.2 MB/s \nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n     |████████████████████████████████| 6.6 MB 56.8 MB/s \nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\nRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\nRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\nInstalling collected packages: tokenizers, transformers\nSuccessfully installed tokenizers-0.12.1 transformers-4.22.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef tokenize(batch):\n    return tokenizer(batch['text'], padding=True, truncation=True)\n\n\nprint(tokenize(emotions['train'][0:2]))\n\n{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n\n\n\nemotions_encoded = emotions.map(tokenize, batched = True, batch_size = None)\n\n\n\n\n\n\n\n\n\n\n\nfrom transformers import TFAutoModelForSequenceClassification\n\nnum_labels = 6\n\ntf_model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\ntf_model\n\n\n\n\nSome layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_transform', 'activation_13', 'vocab_projector']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'pre_classifier', 'classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification at 0x7f50d851c510>\n\n\n\nfrom sklearn.metrics import accuracy_score, f1_score\n\n\ntokenizer_columns = tokenizer.model_input_names\n\n\nbatch_size = 64\n\n\ntf_train_dataset = emotions_encoded['train'].to_tf_dataset(columns = tokenizer_columns, \n                                                           label_cols = ['label'], \n                                                           shuffle=True, batch_size=batch_size)\n\ntf_validation_dataset = emotions_encoded['validation'].to_tf_dataset(columns = tokenizer_columns, \n                                                           label_cols = ['label'], \n                                                           shuffle=True, batch_size=batch_size)\n\n\ncallbacks = [PushToHubCallback(\"model_output/\",\n                               tokenizer=tokenizer,\n                               hub_model_id=\"twitter-emotion-classifier-BERT\")]\n\ntf_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), \n                 loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                 metrics = tf.metrics.SparseCategoricalAccuracy())\n\ntf_model.fit(tf_train_dataset, validation_data = tf_validation_dataset, epochs = 2, callbacks=callbacks)\n\nCloning https://huggingface.co/jakegehri/twitter-emotion-classifier-BERT into local empty directory.\nWARNING:huggingface_hub.repository:Cloning https://huggingface.co/jakegehri/twitter-emotion-classifier-BERT into local empty directory.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEpoch 1/2\n  6/250 [..............................] - ETA: 2:05 - loss: 0.1446 - sparse_categorical_accuracy: 0.9245\n\n\nWARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1977s vs `on_train_batch_end` time: 0.3150s). Check your callbacks.\n\n\n250/250 [==============================] - 163s 624ms/step - loss: 0.1101 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.1436 - val_sparse_categorical_accuracy: 0.9345\nEpoch 2/2\n250/250 [==============================] - 136s 545ms/step - loss: 0.0868 - sparse_categorical_accuracy: 0.9599 - val_loss: 0.1442 - val_sparse_categorical_accuracy: 0.9325\n\n\nSeveral commits (2) will be pushed upstream.\nWARNING:huggingface_hub.repository:Several commits (2) will be pushed upstream.\nThe progress bars may be unreliable.\nWARNING:huggingface_hub.repository:The progress bars may be unreliable.\n\n\n\n\n\nremote: Scanning LFS files for validity, may be slow...        \nremote: LFS file scan complete.        \nTo https://huggingface.co/jakegehri/twitter-emotion-classifier-BERT\n   a929610..8b9eebc  main -> main\n\nWARNING:huggingface_hub.repository:remote: Scanning LFS files for validity, may be slow...        \nremote: LFS file scan complete.        \nTo https://huggingface.co/jakegehri/twitter-emotion-classifier-BERT\n   a929610..8b9eebc  main -> main\n\n\n\n<keras.callbacks.History at 0x7f4f960761d0>\n\n\n\ntf_model.push_to_hub(\"twitter-emotion-classifier-BERT\")\n\n\nclassifier = pipeline(\"text-classification\", model = \"jakegehri/twitter-emotion-classifier-BERT\")\n\n\n\n\nSome layers from the model checkpoint at jakegehri/twitter-emotion-classifier-BERT were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at jakegehri/twitter-emotion-classifier-BERT and are newly initialized: ['dropout_98']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_tweet = \"what is going on\"\npreds = classifier(test_tweet, top_k=6)\nlabels = emotions['train'].features['label'].names\nemotion_int = int(preds[0]['label'].replace(\"_\",\" \").split()[1])\nlabels[emotion_int]\n\n'anger'\n\n\n\npreds\n\n[{'label': 'LABEL_3', 'score': 0.6134325861930847},\n {'label': 'LABEL_4', 'score': 0.3628736138343811},\n {'label': 'LABEL_1', 'score': 0.01299766730517149},\n {'label': 'LABEL_0', 'score': 0.008490157313644886},\n {'label': 'LABEL_5', 'score': 0.0016536037437617779},\n {'label': 'LABEL_2', 'score': 0.0005523563013412058}]\n\n\n\nrank = []\n\nfor i in preds:\n  label = i['label']\n  rank.append(int(i['label'].replace(\"_\",\" \").split()[1]))\n\n\nre_rank = []\nfor i in rank:\n  re_rank.append(labels[i])\n\n\nre_rank\n\n['anger', 'fear', 'joy', 'sadness', 'surprise', 'love']\n\n\n\npreds_df = pd.DataFrame(preds)\nplt.bar(re_rank, 100 * preds_df['score'], color = 'C0')\nplt.title(f'\"{test_tweet}\"')\nplt.show()"
  },
  {
    "objectID": "projects/Syllogism Finisher/conclusion_writer.html",
    "href": "projects/Syllogism Finisher/conclusion_writer.html",
    "title": "Syllogism Conclusion Generator with GPT-2",
    "section": "",
    "text": "import torch\nimport pandas as pd\nimport numpy as np\nfrom datasets import load_dataset, Dataset\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\n\nIntroduction\nThis notebook will be an extension of the last notebook that was working to classify whether two premises could be used to generate a valid conclusion. That model used the a BERT architecture and was fine-tuned on the Avicenna syllogism dataset. This notebook will use the same dataset, but instead fine-tune a GPT-2 model to take in two premises as input and generate the corresponding conclusion.\nI had to write custome start, end and pad tokens in order to properly pad each input as to be forced to randomly chop up syllogisms into pieced and create input blocks of equal size.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nfile_name = 'Avicenna_Train.csv'\nmodel_cp = \"gpt2\"\nmax_length = 200\ntokenizer = GPT2Tokenizer.from_pretrained(model_cp, bos_token = '<startoftext>', \n                                          eos_token='<endoftext>', pad_token='<pad>')\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors='pt')\nmodel = GPT2LMHeadModel.from_pretrained(model_cp).to(device)\nmodel.resize_token_embeddings(len(tokenizer))\n\n\n\n\n\n\n\n\n\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\n\n\nEmbedding(50260, 768)\n\n\n\ndef tokenize(batch):\n    return tokenizer(batch['text'], truncation=True, max_length=max_length, padding='max_length')\n\nThe data came in a csv file containing premise 1, premise 2, validity and conclusion. I needed to filter the dataset removing all invalid syllogisms and then combine the premises and conclusions into a single string for fine-tuning. I found out that telling the model which premise was which and where the conclusion started improved training. Additionally, adding a $ after the last premise slightly imporved training, but this was moreso done to replicate what as done in the original GPT paper.\n\ndef prepare_dataset(file_name):\n    dataset = load_dataset('csv', data_files=file_name, sep = ',', encoding = 'ISO-8859-1')\n    dataset.set_format(type='pandas')\n    df = dataset['train'][:]\n    df = df[df['Syllogistic relation'] == 'yes']\n    df['text'] = '<startoftext>' + 'Premise 1: ' + df['Premise 1'] + 'Premise 2:' + df['Premise 2'] + '$' + 'Conclusion:' + df['Conclusion'] + '<endoftext>'\n    df.reset_index(drop=True, inplace=True)\n    df = df[['text']]\n    dataset = Dataset.from_pandas(df)\n    dataset = dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])\n    return dataset\n\n\ntrain_dataset = prepare_dataset('Avicenna_Train.csv')\ntest_dataset = prepare_dataset('Avicenna_Test.csv')\n\nUsing custom data configuration default-9e35c2288d530357\nReusing dataset csv (/root/.cache/huggingface/datasets/csv/default-9e35c2288d530357/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n\n\n\n\n\n       \n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nUsing custom data configuration default-80959f65edc13f7a\nReusing dataset csv (/root/.cache/huggingface/datasets/csv/default-80959f65edc13f7a/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntokenizer.decode(train_dataset['input_ids'][0])\n\n'<startoftext> Premise 1: Chronic diseases are heart attacks and stroke, cancer such as breast and colon cancer, diabetes, epilepsy and seizures, obesity, and oral health problems.Premise 2:In populations that eat a regular high-fiber diet of more than 50 grams of fiber per dayTrusted Source, like rural South Africans, chronic diseases are very low.$Conclusion:In populations that eat a regular high-fiber diet of more than 50 grams of fiber per dayTrusted Source, like rural South Africans, heart attacks and stroke, cancer such as breast and colon cancer, diabetes, epilepsy and seizures, obesity, and oral health problems are very low. <endoftext> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'\n\n\n\n\nTraining\n\nmodel_name = model_cp.split(\"/\")[-1]\ntraining_args = TrainingArguments(\n    f\"{model_cp}-finetuned-syllogism\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    push_to_hub=False,\n)\n\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    data_collator=data_collator\n)\n\n\ntrainer.train()\n\n/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 2427\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 912\n\n\n\n\n    \n      \n      \n      [912/912 04:18, Epoch 3/3]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n    \n  \n  \n    \n      1\n      No log\n      2.311477\n    \n    \n      2\n      2.267000\n      2.312688\n    \n    \n      3\n      2.267000\n      2.314481\n    \n  \n\n\n\n***** Running Evaluation *****\n  Num examples = 630\n  Batch size = 8\nSaving model checkpoint to gpt2-finetuned-syllogism/checkpoint-500\nConfiguration saved in gpt2-finetuned-syllogism/checkpoint-500/config.json\nModel weights saved in gpt2-finetuned-syllogism/checkpoint-500/pytorch_model.bin\n***** Running Evaluation *****\n  Num examples = 630\n  Batch size = 8\n***** Running Evaluation *****\n  Num examples = 630\n  Batch size = 8\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\nTrainOutput(global_step=912, training_loss=2.215385637785259, metrics={'train_runtime': 258.8719, 'train_samples_per_second': 28.126, 'train_steps_per_second': 3.523, 'total_flos': 743151283200000.0, 'train_loss': 2.215385637785259, 'epoch': 3.0})\n\n\n\nimport math\neval_results = trainer.evaluate()\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n\n***** Running Evaluation *****\n  Num examples = 630\n  Batch size = 8\n\n\n\n    \n      \n      \n      [79/79 00:05]\n    \n    \n\n\nPerplexity: 10.12\n\n\n\n\nTesting\nFirst a classic example\n\ntest = 'Premise 1: All men are mortal. Premise 2: Socrates is a man. $ Conclusion: '\n\n\ninput_ids = tokenizer(test, return_tensors='pt')['input_ids'].to(device)\n\n\noutput_greedy = model.generate(input_ids, max_length=25)\ntokenizer.decode(output_greedy[0])\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n'Premise 1: All men are mortal. Premise 2: Socrates is a man. $ Conclusion:  Socrates is mortal'\n\n\nI dont know why it is generating this error, I believe it has something to do with me changing the models innate tokens in the beginning. But we can see that the model was able to accurately generate the conclusion for this syllogism. This first example is using greedy search, where our model simply makes a next word prediction based on the our probability distribution over the vocabulary.\n\noutput_beam = model.generate(input_ids, max_length=25, num_beams=5)\ntokenizer.decode(output_beam[0])\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n'Premise 1: All men are mortal. Premise 2: Socrates is a man. $ Conclusion:  Socrates is mortal'\n\n\nBeam search is when the model generates n number of ‘beams’ or full sentence predictions (in this case 5) and then a word is decided based on highest probability and we continue moving down the rest of the sentence, not going back to earlier ones. This model also looks good. Beam will usually outperform greedy.\n\n\nThe End\nSee below for more tests and search methods.\n\noutput_temp = model.generate(input_ids, max_length=25, do_sample=True, temperature = 0.5)\ntokenizer.decode(output_temp[0])\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n'Premise 1: All men are mortal. Premise 2: Socrates is a man. $ Conclusion:  Socrates is mortal'\n\n\n\noutput_topk = model.generate(input_ids, max_length=25, do_sample=True, top_k=50)\ntokenizer.decode(output_topk[0])\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n'Premise 1: All men are mortal. Premise 2: Socrates is a man. $ Conclusion:  Socrates is a'\n\n\n\noutput_topp = model.generate(input_ids, max_length=25, do_sample=True, top_p=0.90)\ntokenizer.decode(output_topp[0])\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n'Premise 1: All men are mortal. Premise 2: Socrates is a man. $ Conclusion:  Socrates is mortal'\n\n\n\ntest2 = 'Premise 1: All mammals are animals. Premise 2: All elephants are mammals. $ Conclusion: '\n\n\ninput_ids = tokenizer(test2, return_tensors='pt')['input_ids'].to(device)\n\n\noutput_greedy = model.generate(input_ids, max_length = 50)\ntokenizer.decode(output_greedy[0])\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n'Premise 1: All mammals are animals. Premise 2: All elephants are mammals. $ Conclusion:  All elephants are animals. <endoftext>                       '\n\n\n\noutput_beam = model.generate(input_ids, max_length = 50, num_beams=5)\ntokenizer.decode(output_beam[0])\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n'Premise 1: All mammals are animals. Premise 2: All elephants are mammals. $ Conclusion:  All elephants are animals. <endoftext>                       '\n\n\n\ntest3 = 'Premise 1: All mammals are warm-blooded. Premise 2: All black dogs are mammals. $ Conclusion: '\ninput_ids = tokenizer(test3, return_tensors='pt')['input_ids'].to(device)\noutput_beam = model.generate(input_ids, max_length=40, num_beams = 5)\ntokenizer.decode(output_beam[0])\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n'Premise 1: All mammals are warm-blooded. Premise 2: All black dogs are mammals. $ Conclusion:  All black dogs are warm-blooded. <endoftext>   <endoftext>  animal is warm-'"
  },
  {
    "objectID": "projects/DNN From Scratch/no_hands.html",
    "href": "projects/DNN From Scratch/no_hands.html",
    "title": "DNN From Scratch: The nuts and bolts",
    "section": "",
    "text": "import torch\nimport pandas as pd\nimport numpy as np\nimport torch.nn.functional as F\nfrom fastai import *\nfrom fastbook import *\n\n\ndata = pd.read_csv('train.csv')\n\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n  \n\n\n\n\n\ndata.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\nmodes = data.mode().iloc[0]\nmodes\n\nPassengerId                      1\nSurvived                       0.0\nPclass                         3.0\nName           Abbing, Mr. Anthony\nSex                           male\nAge                           24.0\nSibSp                          0.0\nParch                          0.0\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object\n\n\n\ndata.fillna(modes, inplace=True)\n\n\ndata.isna().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64\n\n\n\ndata.describe(include=(np.number))\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Age\n      SibSp\n      Parch\n      Fare\n    \n  \n  \n    \n      count\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n    \n    \n      mean\n      446.000000\n      0.383838\n      2.308642\n      28.566970\n      0.523008\n      0.381594\n      32.204208\n    \n    \n      std\n      257.353842\n      0.486592\n      0.836071\n      13.199572\n      1.102743\n      0.806057\n      49.693429\n    \n    \n      min\n      1.000000\n      0.000000\n      1.000000\n      0.420000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      223.500000\n      0.000000\n      2.000000\n      22.000000\n      0.000000\n      0.000000\n      7.910400\n    \n    \n      50%\n      446.000000\n      0.000000\n      3.000000\n      24.000000\n      0.000000\n      0.000000\n      14.454200\n    \n    \n      75%\n      668.500000\n      1.000000\n      3.000000\n      35.000000\n      1.000000\n      0.000000\n      31.000000\n    \n    \n      max\n      891.000000\n      1.000000\n      3.000000\n      80.000000\n      8.000000\n      6.000000\n      512.329200\n    \n  \n\n\n\n\n\ndata.describe(include=object)\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Ticket\n      Cabin\n      Embarked\n    \n  \n  \n    \n      count\n      891\n      891\n      891\n      891\n      891\n    \n    \n      unique\n      891\n      2\n      681\n      147\n      3\n    \n    \n      top\n      Braund, Mr. Owen Harris\n      male\n      347082\n      B96 B98\n      S\n    \n    \n      freq\n      1\n      577\n      7\n      691\n      646\n    \n  \n\n\n\n\n\ndata['Fare'].hist()\n\n<AxesSubplot:>\n\n\n\n\n\n\ndata['LogFare'] = np.log(data['Fare'] + 1)\n\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n      LogFare\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      B96 B98\n      S\n      2.110213\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n      4.280593\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      B96 B98\n      S\n      2.188856\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n      3.990834\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      B96 B98\n      S\n      2.202765\n    \n  \n\n\n\n\n\ndata = pd.get_dummies(data, columns = ['Pclass', 'Sex', 'Embarked'])\n\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Name\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      LogFare\n      Pclass_1\n      Pclass_2\n      Pclass_3\n      Sex_female\n      Sex_male\n      Embarked_C\n      Embarked_Q\n      Embarked_S\n    \n  \n  \n    \n      0\n      1\n      0\n      Braund, Mr. Owen Harris\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      B96 B98\n      2.110213\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n    \n    \n      1\n      2\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      4.280593\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n    \n    \n      2\n      3\n      1\n      Heikkinen, Miss. Laina\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      B96 B98\n      2.188856\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n    \n    \n      3\n      4\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      3.990834\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      4\n      5\n      0\n      Allen, Mr. William Henry\n      35.0\n      0\n      0\n      373450\n      8.0500\n      B96 B98\n      2.202765\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n    \n  \n\n\n\n\n\ndep_var = ['Survived']\n\n\nindep_vars = ['Age', 'SibSp', 'Parch', 'LogFare', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n\n\nlen(indep_vars)\n\n12\n\n\n\ny = torch.tensor(data[dep_var].values, dtype=torch.float)\n\n\nX = torch.tensor(data[indep_vars].values, dtype=torch.float)\n\n\nvals, indicies = X.max(dim=0)\n\n\nX = X / vals\n\n\ntrn_split, val_split = RandomSplitter(seed=42)(X)\n\n\nlen(trn_split), len(val_split)\n\n(713, 178)\n\n\n\nX_train, y_train = X[trn_split], y[trn_split]\nX_val, y_val = X[val_split], y[val_split]\n\n\nnips = X_train.shape[1]\n\n\ntorch.manual_seed(42)\n\ndef get_coeffs(nips = nips, l1_size = 20, n_classes = 1):\n    layer1 = (torch.rand(nips, l1_size)-0.5) / nips\n    layer2 = (torch.rand(l1_size, n_classes)-0.5)\n    const = torch.rand(1)[0]\n    return layer1.requires_grad_(), layer2.requires_grad_(), const.requires_grad_()\n\n\ndef forward_pass(coeffs, X_train):\n    l1, l2, const = coeffs\n    acts = F.relu(X_train@l1)\n    acts = acts@l2 + const\n    return torch.sigmoid(acts)\n\n\ndef calc_loss(acts, y_train): return torch.abs(acts - y_train).mean()\n\n\ndef backprop(coeffs, lr):\n    for layer in coeffs:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\n\ndef one_epoch(coeffs, lr):\n    acts = forward_pass(coeffs, X_train)\n    loss = calc_loss(acts, y_train)\n    loss.backward()\n    with torch.no_grad(): backprop(coeffs, lr)\n    print(f\"{loss:.3f}\", end = \"; \")\n\n\ndef acc(coeffs): return (y_val.bool()==(forward_pass(coeffs, X_val)>0.5)).float().mean()\n\n\ndef train_model(epochs=50, lr=2):\n    torch.manual_seed(42)\n    coeffs = get_coeffs()\n    for i in range(epochs): one_epoch(coeffs, lr)\n    return coeffs, acc(coeffs)\n    \n\n\n_, acc = train_model()\n\n0.548; 0.529; 0.503; 0.466; 0.408; 0.357; 0.330; 0.313; 0.298; 0.286; 0.277; 0.269; 0.261; 0.255; 0.249; 0.244; 0.239; 0.235; 0.231; 0.229; 0.226; 0.224; 0.222; 0.220; 0.219; 0.217; 0.216; 0.215; 0.214; 0.213; 0.212; 0.211; 0.211; 0.210; 0.209; 0.209; 0.208; 0.207; 0.207; 0.206; 0.206; 0.205; 0.205; 0.204; 0.204; 0.204; 0.203; 0.203; 0.203; 0.202; \n\n\n\nacc\n\ntensor(0.8258)"
  },
  {
    "objectID": "projects/Trailcam/trailcam_model.html",
    "href": "projects/Trailcam/trailcam_model.html",
    "title": "Trail Cam Deer, Elk and Moose Classifier",
    "section": "",
    "text": "import fastbook\nfrom fastbook import *\nfrom fastai.vision.all import *\nfrom fastai.vision.widgets import *\nimport timm\nfrom duckduckgo_search import ddg_images\nfrom time import sleep\nimport os\nimport gradio\n\n\ndef search_images(term, max_images=100):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n\nsearches = 'moose', 'deer', 'elk'\npath = Path('trail_cam')\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls = search_images(f'{o} trail cam photo'))\n    sleep(10)\n    download_images(dest, urls = search_images(f'{o} trail cam photo day'))\n    sleep(10)\n    download_images(dest, urls = search_images(f'{o} trail cam photo night'))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'moose trail cam photo'\nSearching for 'moose trail cam photo day'\nSearching for 'moose trail cam photo night'\nSearching for 'deer trail cam photo'\nSearching for 'deer trail cam photo day'\nSearching for 'deer trail cam photo night'\nSearching for 'elk trail cam photo'\nSearching for 'elk trail cam photo day'\nSearching for 'elk trail cam photo night'\n\n\n\nfns = get_image_files(path)\nlen(fns)\n\n877\n\n\n\nfailed = verify_images(fns)\nfailed.map(Path.unlink)\nfns = get_image_files(path)\nlen(failed), len(fns)\n\n(12, 865)\n\n\n\nos.getcwd()\nfor i, filename in enumerate(os.listdir(path/'deer')):\n   os.rename(\"trail_cam/deer/\" + filename, \"trail_cam/deer/deer_\" + str(i) + \".jpg\")\nfor i, filename in enumerate(os.listdir(path/'moose')):\n   os.rename(\"trail_cam/moose/\" + filename, \"trail_cam/moose/moose_\" + str(i) + \".jpg\")\nfor i, filename in enumerate(os.listdir(path/'elk')):\n   os.rename(\"trail_cam/elk/\" + filename, \"trail_cam/elk/elk_\" + str(i) + \".jpg\")\n\n\ndls = ImageDataLoaders.from_name_func('.',\n    get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=RegexLabeller(pat = r'^([^/]+)_\\d+'),\n    item_tfms=Resize(224))\n\n\ndls.valid.show_batch(max_n=4)\n\n\n\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(5)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      2.002779\n      1.369467\n      0.586207\n      00:07\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.426437\n      1.002604\n      0.425287\n      00:03\n    \n    \n      1\n      1.082575\n      0.859882\n      0.413793\n      00:03\n    \n    \n      2\n      0.907559\n      0.716293\n      0.333333\n      00:03\n    \n    \n      3\n      0.748531\n      0.688598\n      0.252874\n      00:03\n    \n    \n      4\n      0.647351\n      0.693779\n      0.264368\n      00:03\n    \n  \n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(6, nrows=1)\n\n\n\n\n\n\n\n\n\n\n\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\n    \nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n    \ndls = ImageDataLoaders.from_name_func('.',\n    get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=RegexLabeller(pat = r'^([^/]+)_\\d+'),\n    item_tfms=Resize(224))\n\n\nlearn = vision_learner(dls, 'convnext_tiny_in22k', metrics=error_rate).to_fp16()\nlearn.fine_tune(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.692869\n      3.532001\n      0.581395\n      00:03\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.983810\n      1.380556\n      0.313953\n      00:03\n    \n    \n      1\n      0.885328\n      0.899624\n      0.232558\n      00:03\n    \n    \n      2\n      0.792400\n      0.785097\n      0.255814\n      00:03\n    \n    \n      3\n      0.720126\n      0.789632\n      0.197674\n      00:03\n    \n    \n      4\n      0.623536\n      0.744941\n      0.186047\n      00:03\n    \n    \n      5\n      0.556991\n      0.737408\n      0.209302\n      00:03\n    \n    \n      6\n      0.512330\n      0.754599\n      0.197674\n      00:03\n    \n    \n      7\n      0.464188\n      0.772003\n      0.197674\n      00:03\n    \n    \n      8\n      0.431808\n      0.771565\n      0.197674\n      00:03\n    \n    \n      9\n      0.404420\n      0.780129\n      0.197674\n      00:03\n    \n  \n\n\n\n\nfrom fastdownload import download_url\nurls = search_images_ddg('trail cam deer')\nurls[0]\ndownload_url(urls[2], 'deer.jpg')\n\nim = Image.open('deer.jpg')\nim.to_thumb(190,190)\n\n\n\n\n\n\n    \n      \n      100.58% [196608/195476 00:00<00:00]\n    \n    \n\n\n\n\n\n\nurls = search_images_ddg('trail cam elk')\nurls[0]\ndownload_url(urls[0], 'elk.jpg')\n\nim = Image.open('elk.jpg')\nim.to_thumb(190,190)\n\n\n\n\n\n\n    \n      \n      116.39% [40960/35192 00:00<00:00]\n    \n    \n\n\n\n\n\n\nurls = search_images_ddg('trail cam moose')\nurls[1]\ndownload_url(urls[1], 'moose.jpg')\n\nim = Image.open('moose.jpg')\nim.to_thumb(190,190)\n\n\n\n\n\n\n    \n      \n      100.81% [270336/268163 00:00<00:00]\n    \n    \n\n\n\n\n\n\nim = PILImage.create(\"deer.jpg\")\nim.thumbnail((192, 192))\nim\n\n\n\n\n\nlearn.predict(im)\n\n\n\n\n\n\n\n\n('deer', TensorBase(0), TensorBase([0.6719, 0.0927, 0.2354]))\n\n\n\nPickel and export model to be uploaded to gradio\n\nlearn.export('model.pkl')\n\n\nfrom fastai.vision.all import *\nimport gradio as gr\n\nlearn = load_learner('model.pkl')\ncategories = learn.dls.vocab\n\ndef classify_image(img):\n    img = PILImage.create(img)\n    pred, idx, probs = learn.predict(img)\n    return dict(zip(categories, map(float,probs)))\n\n\nimage = gr.inputs.Image(shape=(192,192))\nlabel = gr.outputs.Label()\nexamples = ['deer.jpg', 'elk.jpg', 'moose.jpg']\n\nintf = gr.Interface(fn=classify_image, inputs=image, outputs=label, examples=examples)\nintf.launch(inline=False, share=True)\n\n/usr/local/lib/python3.9/dist-packages/gradio/inputs.py:256: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n  warnings.warn(value)\n/usr/local/lib/python3.9/dist-packages/gradio/outputs.py:196: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n  warnings.warn(value)\n\n\nRunning on local URL:  http://127.0.0.1:7860\nRunning on public URL: https://27270.gradio.app\n\nThis share link expires in 72 hours. For free permanent hosting, check out Spaces: https://huggingface.co/spaces\n\n\n(<gradio.routes.App at 0x7f8110cc3040>,\n 'http://127.0.0.1:7860/',\n 'https://27270.gradio.app')"
  },
  {
    "objectID": "projects/Stable Diffusion/stable-diffusion-from-scratch.html",
    "href": "projects/Stable Diffusion/stable-diffusion-from-scratch.html",
    "title": "Stable Diffusion From Scratch",
    "section": "",
    "text": "! pip install diffusers\n\nRequirement already satisfied: diffusers in /usr/local/lib/python3.9/dist-packages (0.9.0)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.9/dist-packages (from diffusers) (4.12.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from diffusers) (3.7.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from diffusers) (2.28.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from diffusers) (2022.7.9)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from diffusers) (9.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from diffusers) (1.23.1)\nRequirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from diffusers) (0.11.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers) (21.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers) (4.64.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers) (4.3.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers) (5.4.1)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata->diffusers) (3.8.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers) (1.26.10)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->diffusers) (2019.11.28)\nRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers) (2.1.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.9->huggingface-hub>=0.10.0->diffusers) (3.0.9)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\nfrom huggingface_hub import notebook_login\nfrom transformers import CLIPTokenizer, CLIPTextModel\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom diffusers import LMSDiscreteScheduler\nimport torch\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom PIL import Image\n\n\nnotebook_login()\n\n\n\n\n\nclip_model_cp = 'openai/clip-vit-large-patch14'\nvae_cp = 'stabilityai/sd-vae-ft-ema'\nunet_cp = 'CompVis/stable-diffusion-v1-4'\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbeta_start, beta_end = 0.00085, 0.012\n\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n\nftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'logit_scale', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight']\n- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\nvae = AutoencoderKL.from_pretrained(vae_cp).to(device)\nunet = UNet2DConditionModel.from_pretrained(unet_cp, subfolder = 'unet').to(device)\n\nCannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \npip install accelerate\n.\nCannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \npip install accelerate\n.\n\n\n\nscheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule='scaled_linear', num_train_timesteps=1000)\n\n\nprompt = ['a painting of George Washington and Steve Jobs']\n\nheight = 512\nwidth = 512\nnum_inference_steps = 50\nguidence_scale = 7.5\nbatch_size = 1\n\n\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ntext_input['input_ids']\n\ntensor([[49406,   320,  3086,   539,  3296,  4365,   537,  3803,  3735, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\n\n\n\ntext_input['attention_mask']\n\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]])\n\n\n\ntokenizer.decode(text_input['input_ids'][0])\n\n'<|startoftext|>a painting of george washington and steve jobs <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'\n\n\n\ntext_embeddings = text_encoder(text_input.input_ids.to(device))[0]\ntext_embeddings.shape\n\ntorch.Size([1, 77, 768])\n\n\n\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer([\"\"]*batch_size, padding='max_length', max_length=max_length, return_tensors = 'pt')\nuncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\nuncond_embeddings.shape\n\ntorch.Size([1, 77, 768])\n\n\n\nembeddings = torch.cat([uncond_embeddings, text_embeddings])\nembeddings.shape\n\ntorch.Size([2, 77, 768])\n\n\n\nlatents = torch.randn((batch_size, unet.in_channels, height//8, width//8)).to(device)\nlatents.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\n\nscheduler.set_timesteps(num_inference_steps)\n\n\nlatents = latents * scheduler.init_noise_sigma\n\n\nscheduler.timesteps\n\ntensor([999.0000, 978.6122, 958.2245, 937.8367, 917.4490, 897.0612, 876.6735,\n        856.2857, 835.8980, 815.5102, 795.1224, 774.7347, 754.3469, 733.9592,\n        713.5714, 693.1837, 672.7959, 652.4082, 632.0204, 611.6327, 591.2449,\n        570.8571, 550.4694, 530.0816, 509.6939, 489.3061, 468.9184, 448.5306,\n        428.1429, 407.7551, 387.3673, 366.9796, 346.5918, 326.2041, 305.8163,\n        285.4286, 265.0408, 244.6531, 224.2653, 203.8776, 183.4898, 163.1020,\n        142.7143, 122.3265, 101.9388,  81.5510,  61.1633,  40.7755,  20.3878,\n          0.0000], dtype=torch.float64)\n\n\n\nscheduler.sigmas\n\ntensor([14.6146, 12.9368, 11.4916, 10.2429,  9.1604,  8.2187,  7.3972,  6.6780,\n         6.0465,  5.4903,  4.9989,  4.5633,  4.1761,  3.8308,  3.5221,  3.2451,\n         2.9958,  2.7709,  2.5673,  2.3825,  2.2143,  2.0606,  1.9199,  1.7907,\n         1.6716,  1.5617,  1.4598,  1.3651,  1.2768,  1.1944,  1.1171,  1.0444,\n         0.9759,  0.9112,  0.8497,  0.7913,  0.7355,  0.6820,  0.6306,  0.5809,\n         0.5328,  0.4858,  0.4397,  0.3940,  0.3483,  0.3019,  0.2535,  0.2012,\n         0.1393,  0.0292,  0.0000])\n\n\n\nfor i, t in enumerate(tqdm(scheduler.timesteps)):\n    inp = torch.cat([latents] * 2)\n    inp = scheduler.scale_model_input(inp, t)\n    \n    with torch.no_grad():\n        pred = unet(inp, t, encoder_hidden_states=embeddings).sample\n        \n    pred_uncond, pred_text = pred.chunk(2)\n    pred = pred_uncond + guidence_scale * (pred_text - pred_uncond)\n    \n    latents = scheduler.step(pred, t, latents).prev_sample\n\n\n\n\n\nplt.imshow(pred.squeeze(0).permute(1,2,0).cpu().numpy())\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n<matplotlib.image.AxesImage at 0x7f825e0d4b20>\n\n\n\n\n\n\nplt.imshow(latents.squeeze(0).permute(1,2,0).cpu().numpy())\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n<matplotlib.image.AxesImage at 0x7f825e046a90>\n\n\n\n\n\n\nwith torch.no_grad():\n    image = vae.decode(1 / 0.18215 * latents).sample\n\n\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image[0].detach().cpu().permute(1, 2, 0).numpy()\nimage = (image * 255).round().astype('uint8')\nImage.fromarray(image)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Paper Replication: Vision Transformer\n\n\n\n\n\n\n\nPython\n\n\nDeep Learning\n\n\nComputer Vision\n\n\n\n\nThe first in a series of paper replications that will collectively lay the foundation for a full replication of the stable diffusion model from scratch.\n\n\n\n\n\n\nDec 9, 2022\n\n\nJake Gehri\n\n\n\n\n\n\n  \n\n\n\n\nStable Diffusion From Scratch\n\n\n\n\n\n\n\nPython\n\n\nDeep Learning\n\n\n\n\nUsing huggingface components to build a stable diffusion pipeline from scratch. Used fast.ai project as inspiration.\n\n\n\n\n\n\nDec 6, 2022\n\n\nJake Gehri\n\n\n\n\n\n\n  \n\n\n\n\nSyllogism Conclusion Generator with GPT-2\n\n\n\n\n\n\n\nPython\n\n\nDeep Learning\n\n\nNLP\n\n\n\n\nGiven two premises that form a valid syllogism, this autoregressive model can accurately complete the syllogism by generating a conclusion.\n\n\n\n\n\n\nNov 29, 2022\n\n\nJake Gehri\n\n\n\n\n\n\n  \n\n\n\n\nSyllogism Validation with BERT\n\n\n\n\n\n\n\nPython\n\n\nDeep Learning\n\n\nNLP\n\n\n\n\nGiven two premises this validation model can classify validity with 85% accuracy on a 50/50 split dataset.\n\n\n\n\n\n\nNov 27, 2022\n\n\nJake Gehri\n\n\n\n\n\n\n  \n\n\n\n\nCounting Cards: Computer vision classifier for playing cards\n\n\n\n\n\n\n\nPython\n\n\nDeep Learning\n\n\nComputer Vision\n\n\n\n\nA PyTorch playing card multi-label classifier from scratch. Basic CNN, EfficientNet with & without augmentation. Achieves 98% accuracy.\n\n\n\n\n\n\nNov 16, 2022\n\n\nJake Gehri\n\n\n\n\n\n\n  \n\n\n\n\nDNN From Scratch: The nuts and bolts\n\n\n\n\n\n\n\nPython\n\n\nDeep Learning\n\n\n\n\nAchieving 83% accuracy on the Titanic Kaggle competition dataset with a DNN built from scratch.\n\n\n\n\n\n\nNov 1, 2022\n\n\nJake Gehri\n\n\n\n\n\n\n  \n\n\n\n\nMulti-label Classification of Amazonian Land Space\n\n\n\n\n\n\n\nPython\n\n\nDeep Learning\n\n\nComputer Vision\n\n\n\n\nProject\n\n\n\n\n\n\nOct 19, 2022\n\n\nJake Gehri\n\n\n\n\n\n\n  \n\n\n\n\nTrail Cam Deer, Elk and Moose Classifier\n\n\n\n\n\n\n\nPython\n\n\nDeep Learning\n\n\nComputer Vision\n\n\n\n\nProject\n\n\n\n\n\n\nOct 12, 2022\n\n\nJake Gehri\n\n\n\n\n\n\n  \n\n\n\n\nTwitter Bot: NLP Emotion Classifier\n\n\n\n\n\n\n\nPython\n\n\nDeep Learning\n\n\nNLP\n\n\n\n\nBuilding and deploying an emotion classifying twitter bot that responds to users who prompt the bot with a # of interest. Bot uses a pretrained BERT encoder fine tuned on a tweet emotion dataset.\n\n\n\n\n\n\nOct 7, 2022\n\n\nJake Gehri\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jake Gehri",
    "section": "",
    "text": "My name is Jake Gehri. I am from Buckley, WA and currently reside in New York City, where I work as an Investment Banking Analyst for Rothschild & Co. I graduated from Yale University in 2022 where I majored in Economics and Global affairs, and played on the Yale Baseball team.\nI am interested in Artificial Intelligence and am self-taught in (mostly) everything I know about it. I am looking to pursue graduate studies in computer science (artificial intelligence) in the near future.\n\n\n\nYale University New Haven, CT\nB.A. in Economics | August 2018 - May 2022  B.A. in Global Affairs | August 2018 - May 2022\n\n\n\nRothschild & Co., New York, NY | July 2022 – Present Analyst, Investment Banking Division, Financial Sponsors Group \nPharos Global Health Advisors, Boston, MA | August 2021 – January 2022  Financial Consultant\n\n\n\nSyllogism Validation with BERT. Given two premises this validation model can classify validity with 85% accuracy on a 50/50 split dataset. \nCounting Cards: Computer vision classifier for playing cards. A PyTorch playing card multi-label classifier from scratch. Basic CNN, EfficientNet with & without augmentation. Achieves 98% accuracy."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects/ViT/vision-transformer.html",
    "href": "projects/ViT/vision-transformer.html",
    "title": "Paper Replication: Vision Transformer",
    "section": "",
    "text": "import torch\nimport torchvision\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torchmetrics import Accuracy\nfrom torchvision import transforms\nfrom torchinfo import summary\n\n\ntrain_split_len = 1000\ntrainset = torchvision.datasets.CIFAR10(root = 'data', train = True, download=True, transform = ToTensor())\npart_train = torch.utils.data.random_split(trainset, [train_split_len, len(trainset)-train_split_len])[0]\ntrainloader = DataLoader(part_train, batch_size=25, shuffle=True)\n\nFiles already downloaded and verified\n\n\n\ntest_split_len = 200\ntestset = torchvision.datasets.CIFAR10(root = 'data', train = False, download=True, transform = ToTensor())\npart_test = torch.utils.data.random_split(testset, [test_split_len, len(testset)-test_split_len])[0]\ntestloader = DataLoader(part_test, batch_size=25, shuffle=True)\n\nFiles already downloaded and verified\n\n\n\nclasses = testset.class_to_idx\nclasses\n\n{'airplane': 0,\n 'automobile': 1,\n 'bird': 2,\n 'cat': 3,\n 'deer': 4,\n 'dog': 5,\n 'frog': 6,\n 'horse': 7,\n 'ship': 8,\n 'truck': 9}\n\n\n\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n\nimages.shape, labels.shape\n\n(torch.Size([25, 3, 32, 32]), torch.Size([25]))\n\n\n\nprint(list(classes.keys()) [list(classes.values()).index(1)])\nplt.imshow(images[1].permute(1,2,0).numpy())\n\nautomobile\n\n\n<matplotlib.image.AxesImage at 0x7f28e9e29700>\n\n\n\n\n\n\nclass LinearProjectionBlock(nn.Module):\n    def __init__(self, in_channels = 3, patch_size = 2, embedding_dim = 768, batch_size=25, num_patches=256):\n        super().__init__()\n        self.patch = nn.Conv2d(in_channels=in_channels, out_channels=embedding_dim, kernel_size=patch_size, stride=patch_size)\n        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n        \n        self.class_token = nn.Parameter(torch.rand(batch_size, 1, embedding_dim), requires_grad=True)\n        \n        self.positional = nn.Parameter(torch.rand(batch_size, num_patches+1, embedding_dim), requires_grad=True)\n        \n        \n    def forward(self, x):\n        \n        x = self.patch(x)\n        x = self.flatten(x)\n        x = x.permute(0, 2, 1)\n        x = torch.cat((self.class_token, x), dim=1)\n        x = self.positional + x\n        return x\n        \n\n\nclass MSABlock(nn.Module):\n    def __init__(self, embedding_dim = 768, num_heads=12):\n        super().__init__()\n        self.normalize = nn.LayerNorm(normalized_shape=embedding_dim)\n        self.msa = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads = num_heads, batch_first=True)\n        \n    def forward(self, x):\n        x = self.normalize(x)\n        x, _ = self.msa(query=x, key=x, value=x, need_weights=False)\n        return x\n\n\nlin_projs = LinearProjectionBlock(in_channels=3, patch_size=2, embedding_dim=768, batch_size=25, num_patches=256)\n\n\nprojs = lin_projs(images)\n\n\nprojs.shape\n\ntorch.Size([25, 257, 768])\n\n\n\nmsa_block = MSABlock(embedding_dim=768, num_heads=12)\n\n\nout = msa_block(projs)\n\n\nout.shape\n\ntorch.Size([25, 257, 768])\n\n\n\nclass MLPBlock(nn.Module):\n    def __init__(self, embedding_dim = 768, mlp_size = 3072):\n        super().__init__()\n        self.normalize = nn.LayerNorm(embedding_dim)\n        \n        self.mlp = nn.Sequential(\n            nn.Linear(in_features=embedding_dim, out_features=mlp_size),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(in_features=mlp_size, out_features=embedding_dim),\n            nn.Dropout(0.1)\n        )\n        \n    def forward(self, x):\n        x = self.normalize(x)\n        x = self.mlp(x)\n        return x\n\n\nclass TrasformerEncoderBlock(nn.Module):\n    def __init__(self, embedding_dim = 768, num_heads=12, mlp_size=3072):\n        super().__init__()\n        self.msa_block = MSABlock(embedding_dim = embedding_dim, num_heads=num_heads)\n        \n        self.mlp_block = MLPBlock(embedding_dim = embedding_dim, mlp_size = mlp_size)\n        \n    def forward(self, x):\n        x = self.msa_block(x)\n\n        x = self.mlp_block(x)\n\n        return x\n\n\nmlp = MLPBlock(embedding_dim=768)\n\n\nfout = mlp(out)\n\n\nclass ViT(nn.Module):\n    def __init__(self, img_size = 32, in_channels = 3, patch_size = 2, embedding_dim = 768, \n                 batch_size=25, num_patches=256, mlp_size = 3072, layers = 12, num_heads=12, num_classes=10):\n        super().__init__()\n        \n        self.projections = LinearProjectionBlock(in_channels=in_channels, patch_size=patch_size, embedding_dim=embedding_dim, batch_size=batch_size, num_patches=num_patches)\n        \n        self.transformer_encoder = nn.TransformerEncoderLayer(d_model=embedding_dim, \n                                                             nhead=num_heads, \n                                                             dim_feedforward=mlp_size, \n                                                             dropout=0.1, \n                                                             activation=\"gelu\", \n                                                             batch_first=True, \n                                                             norm_first=True)\n        self.encoder_layer = nn.TransformerEncoder(self.transformer_encoder, num_layers=layers)\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(normalized_shape=embedding_dim),\n            nn.Linear(in_features=embedding_dim, out_features=num_classes)\n        \n        )\n    \n    def forward(self, x, layers=12):\n        x = self.projections(x)\n        x = self.encoder_layer(x)\n        x = self.classifier(x[:, 0])\n        return x\n        \n\n\nvit = ViT(in_channels = 3, patch_size = 4, embedding_dim = 768, batch_size=25, num_patches=64, mlp_size = 2048).to('cuda')\n\n\nsummary(vit, input_size=(25, 3, 32, 32),col_names=[\"input_size\"])\n\n======================================================================\nLayer (type:depth-idx)                        Input Shape\n======================================================================\nViT                                           [25, 3, 32, 32]\n├─LinearProjectionBlock: 1-1                  [25, 3, 32, 32]\n│    └─Conv2d: 2-1                            [25, 3, 32, 32]\n│    └─Flatten: 2-2                           [25, 768, 8, 8]\n├─TransformerEncoder: 1-2                     [25, 65, 768]\n│    └─ModuleList: 2-3                        --\n│    │    └─TransformerEncoderLayer: 3-1      [25, 65, 768]\n│    │    └─TransformerEncoderLayer: 3-2      [25, 65, 768]\n│    │    └─TransformerEncoderLayer: 3-3      [25, 65, 768]\n│    │    └─TransformerEncoderLayer: 3-4      [25, 65, 768]\n│    │    └─TransformerEncoderLayer: 3-5      [25, 65, 768]\n│    │    └─TransformerEncoderLayer: 3-6      [25, 65, 768]\n│    │    └─TransformerEncoderLayer: 3-7      [25, 65, 768]\n│    │    └─TransformerEncoderLayer: 3-8      [25, 65, 768]\n│    │    └─TransformerEncoderLayer: 3-9      [25, 65, 768]\n│    │    └─TransformerEncoderLayer: 3-10     [25, 65, 768]\n│    │    └─TransformerEncoderLayer: 3-11     [25, 65, 768]\n│    │    └─TransformerEncoderLayer: 3-12     [25, 65, 768]\n├─Sequential: 1-3                             [25, 768]\n│    └─LayerNorm: 2-4                         [25, 768]\n│    └─Linear: 2-5                            [25, 768]\n======================================================================\nTotal params: 72,995,850\nTrainable params: 72,995,850\nNon-trainable params: 0\nTotal mult-adds (G): 1.01\n======================================================================\nInput size (MB): 0.31\nForward/backward pass size (MB): 688.90\nParams size (MB): 151.47\nEstimated Total Size (MB): 840.67\n======================================================================\n\n\n\ntest_output = vit(images.to('cuda'))\n\n\ntest_output.shape\n\ntorch.Size([25, 10])\n\n\n\ntorch.argmax(test_output[1]), labels[1]\n\n(tensor(0, device='cuda:0'), tensor(9))\n\n\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(vit.parameters(), lr=1e-3, betas=(0.9, 0.999), weight_decay=0.1)\naccuracy = Accuracy(task=\"multiclass\", num_classes=10)\n\n\ndef train_step(model, dataloader, loss_fn, optimizer):\n    model.train()\n    \n    train_loss, train_acc = 0, 0\n    \n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to('cuda'), y.to('cuda')\n\n        y_pred = model(X)\n\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item() \n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        optimizer.step()\n\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\n\ndef test_step(model, dataloader, loss_fn):\n    \n    model.eval() \n    \n    test_loss, test_acc = 0, 0\n    \n    with torch.inference_mode():\n        for batch, (X, y) in enumerate(dataloader):\n            X, y = X.to('cuda'), y.to('cuda')\n    \n            test_pred_logits = model(X)\n\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n            \n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n            \n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    return test_loss, test_acc\n\n\ndef train(model,train_dataloader,test_dataloader, optimizer, loss_fn, epochs = 50):\n    \n    results = {\"train_loss\": [],\n        \"train_acc\": [],\n        \"test_loss\": [],\n        \"test_acc\": []\n    }\n    \n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer)\n        test_loss, test_acc = test_step(model=model,\n            dataloader=test_dataloader,\n            loss_fn=loss_fn)\n        \n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n    return results\n\n\nresults = train(model=vit, train_dataloader=trainloader, test_dataloader=testloader, optimizer=optimizer, loss_fn=loss_fn, epochs=50)\n\n\n\n\nEpoch: 1 | train_loss: 2.3983 | train_acc: 0.1070 | test_loss: 2.3546 | test_acc: 0.0750\nEpoch: 2 | train_loss: 2.4793 | train_acc: 0.1120 | test_loss: 2.4665 | test_acc: 0.1100\nEpoch: 3 | train_loss: 2.4430 | train_acc: 0.1060 | test_loss: 2.3757 | test_acc: 0.0900\nEpoch: 4 | train_loss: 2.3699 | train_acc: 0.1010 | test_loss: 2.3697 | test_acc: 0.1050\nEpoch: 5 | train_loss: 2.3734 | train_acc: 0.1100 | test_loss: 2.3556 | test_acc: 0.1450\nEpoch: 6 | train_loss: 2.3820 | train_acc: 0.0850 | test_loss: 2.3614 | test_acc: 0.0800\nEpoch: 7 | train_loss: 2.3696 | train_acc: 0.0810 | test_loss: 2.3599 | test_acc: 0.1000\nEpoch: 8 | train_loss: 2.3491 | train_acc: 0.0960 | test_loss: 2.3836 | test_acc: 0.1200\nEpoch: 9 | train_loss: 2.3625 | train_acc: 0.1030 | test_loss: 2.3677 | test_acc: 0.0500\nEpoch: 10 | train_loss: 2.3425 | train_acc: 0.1120 | test_loss: 2.3327 | test_acc: 0.1100\nEpoch: 11 | train_loss: 2.3501 | train_acc: 0.1020 | test_loss: 2.3617 | test_acc: 0.1050\nEpoch: 12 | train_loss: 2.3455 | train_acc: 0.0840 | test_loss: 2.3424 | test_acc: 0.0800\nEpoch: 13 | train_loss: 2.3293 | train_acc: 0.1050 | test_loss: 2.3220 | test_acc: 0.1400\nEpoch: 14 | train_loss: 2.3237 | train_acc: 0.0930 | test_loss: 2.3216 | test_acc: 0.1150\nEpoch: 15 | train_loss: 2.3416 | train_acc: 0.0970 | test_loss: 2.3089 | test_acc: 0.1350\nEpoch: 16 | train_loss: 2.3387 | train_acc: 0.0750 | test_loss: 2.3197 | test_acc: 0.0900\nEpoch: 17 | train_loss: 2.3219 | train_acc: 0.0990 | test_loss: 2.3107 | test_acc: 0.0900\nEpoch: 18 | train_loss: 2.3260 | train_acc: 0.1070 | test_loss: 2.3100 | test_acc: 0.1100\nEpoch: 19 | train_loss: 2.3187 | train_acc: 0.1010 | test_loss: 2.3043 | test_acc: 0.1000\nEpoch: 20 | train_loss: 2.3256 | train_acc: 0.1060 | test_loss: 2.3108 | test_acc: 0.1300\nEpoch: 21 | train_loss: 2.3152 | train_acc: 0.1050 | test_loss: 2.3279 | test_acc: 0.1150\nEpoch: 22 | train_loss: 2.3160 | train_acc: 0.1070 | test_loss: 2.3138 | test_acc: 0.1100\nEpoch: 23 | train_loss: 2.3145 | train_acc: 0.1120 | test_loss: 2.3058 | test_acc: 0.0800\nEpoch: 24 | train_loss: 2.3119 | train_acc: 0.0900 | test_loss: 2.3030 | test_acc: 0.1450\nEpoch: 25 | train_loss: 2.3168 | train_acc: 0.0940 | test_loss: 2.3119 | test_acc: 0.0950\nEpoch: 26 | train_loss: 2.3111 | train_acc: 0.0950 | test_loss: 2.3132 | test_acc: 0.0650\nEpoch: 27 | train_loss: 2.3035 | train_acc: 0.1120 | test_loss: 2.3098 | test_acc: 0.0700\nEpoch: 28 | train_loss: 2.3060 | train_acc: 0.1150 | test_loss: 2.3021 | test_acc: 0.1250\nEpoch: 29 | train_loss: 2.3059 | train_acc: 0.1050 | test_loss: 2.3005 | test_acc: 0.0950\nEpoch: 30 | train_loss: 2.3072 | train_acc: 0.1010 | test_loss: 2.3023 | test_acc: 0.0800\nEpoch: 31 | train_loss: 2.3089 | train_acc: 0.0980 | test_loss: 2.3047 | test_acc: 0.0950\nEpoch: 32 | train_loss: 2.3053 | train_acc: 0.1050 | test_loss: 2.3046 | test_acc: 0.0850\nEpoch: 33 | train_loss: 2.3035 | train_acc: 0.1070 | test_loss: 2.3072 | test_acc: 0.0600\nEpoch: 34 | train_loss: 2.3030 | train_acc: 0.1120 | test_loss: 2.3081 | test_acc: 0.1000\nEpoch: 35 | train_loss: 2.3018 | train_acc: 0.1110 | test_loss: 2.2997 | test_acc: 0.1250\nEpoch: 36 | train_loss: 2.3030 | train_acc: 0.1090 | test_loss: 2.3014 | test_acc: 0.1000\nEpoch: 37 | train_loss: 2.3032 | train_acc: 0.0990 | test_loss: 2.2988 | test_acc: 0.1000\nEpoch: 38 | train_loss: 2.3023 | train_acc: 0.1120 | test_loss: 2.3021 | test_acc: 0.1000\nEpoch: 39 | train_loss: 2.3024 | train_acc: 0.1140 | test_loss: 2.3017 | test_acc: 0.1000\nEpoch: 40 | train_loss: 2.3012 | train_acc: 0.1140 | test_loss: 2.3006 | test_acc: 0.1000\nEpoch: 41 | train_loss: 2.3017 | train_acc: 0.1170 | test_loss: 2.3014 | test_acc: 0.1000\nEpoch: 42 | train_loss: 2.3007 | train_acc: 0.1140 | test_loss: 2.3011 | test_acc: 0.1000\nEpoch: 43 | train_loss: 2.3022 | train_acc: 0.1110 | test_loss: 2.3002 | test_acc: 0.1000\nEpoch: 44 | train_loss: 2.3013 | train_acc: 0.1140 | test_loss: 2.3010 | test_acc: 0.1000\nEpoch: 45 | train_loss: 2.3013 | train_acc: 0.1140 | test_loss: 2.3011 | test_acc: 0.1000\nEpoch: 46 | train_loss: 2.3014 | train_acc: 0.1140 | test_loss: 2.3008 | test_acc: 0.1000\nEpoch: 47 | train_loss: 2.3011 | train_acc: 0.1140 | test_loss: 2.3010 | test_acc: 0.1000\nEpoch: 48 | train_loss: 2.3013 | train_acc: 0.1140 | test_loss: 2.3011 | test_acc: 0.1000\nEpoch: 49 | train_loss: 2.3013 | train_acc: 0.1140 | test_loss: 2.3012 | test_acc: 0.1000\nEpoch: 50 | train_loss: 2.3009 | train_acc: 0.1140 | test_loss: 2.3014 | test_acc: 0.1000\n\n\n\nplt.plot(results['train_loss'])\nplt.plot(results['test_loss'])\n\n\n\n\n\nplt.plot(results['train_acc'])"
  }
]